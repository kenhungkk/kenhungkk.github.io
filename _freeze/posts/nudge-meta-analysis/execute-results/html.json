{
  "hash": "830f38dd8f878a0b63c032023ead572c",
  "result": {
    "markdown": "---\ntitle: Meta-analysis on nudging experiments\ndescription: A recent meta-analysis on choice architecture, or \"nudging\", but people do not seem to agree on if \"nudging\" works.\nauthor: Kenneth Hung\ndate: 2022-08-27\ncategories:\n  - multiple testing\n---\n\n\nAs usual, I am not a psychologist, and do not even run experiments myself, so the model I am discussing here is going to simplistic. But if we are willing to assume a very simple publication bias model, where only statistically significant results are published (a.k.a. \"file-drawer effect\"), then we can apply the methods from [my paper with Will](files/assessing-replicability.pdf), and answer questions such as\n\n- Does any \"nudging\" experiment have real effect? How many of the experiments?\n- What happens if we counter the file-drawer effect with a more stringent threshold?\n\nSo first of all, we are going to only look at *p*-values that are smaller than $0.05$, and adjust them by division by $0.05$, as in our aforementioned paper, or @zhao2018multiple.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(tidyr)\n})\n\nnudge_rawdata <- read.csv(\"https://osf.io/ubt9a/download/\")\n\nnudge_data <- nudge_rawdata %>%\n  select(publication_id, study_id, es_id, cohens_d, variance_d) %>%\n  mutate(\n    est = cohens_d,\n    sd = sqrt(variance_d),\n    z = est / sd,\n    p = 2 * pnorm(-abs(z))\n  )\n\nalpha <- 0.05\nadj_p <- nudge_data$p[nudge_data$p < alpha] / alpha\n\nhist(\n  adj_p, main = \"Histogram of adjusted p-values\", xlab = \"adjusted p-values\"\n)\n```\n\n::: {.cell-output-display}\n![](nudge-meta-analysis_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe first question can be answered by our method, and for convenience, we will just pick $\\lambda$ to be $0.5$. All adjusted p-values will fall in one of these categories:\n\n|                     | Null | Non-null | Total |\n|---------------------|-----:|---------:|------:|\n| Small ($p \\le 0.5$) |    * |        * |     * |\n| Big ($p > 0.5$)     |  $U$ |        * |   $B$ |\n| Total               |  $V$ |        * |   $R$ |\n\nSpecifically, we want to make inference on the false discovery proportion (FDP), which is $V / R$, since every experiment was deemed a discovery beofre. Equivalently, $1 - V/R$ would indicate the proportion of experiments that has an actual effect. Intuitively, if $B$ is not that large, $U$ cannot be large and $V$ cannot be so big either. Formally, we will be using the inequality (2) from the paper:\n\n$$B \\ge U \\ge_{\\text{st}} \\ge \\text{Binomial}(V, 1 - \\lambda),$$\n\nand rejecting unlikely large values of $V$. As a side, we can also get a point estimate of the FDP.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninternal.comparison <- function(pval, lambda) {\n  R <- length(pval)\n  B <- sum(pval > lambda)\n  V_est <- B / (1 - lambda)\n  V <- 0:R\n  V_ucb <- V[max(which(pbinom(B, size = V, prob = 1 - lambda) > 0.05))]\n  data.frame(\n    R = R,\n    V_est = V_est,\n    V_ucb = V_ucb,\n    FDP_est = V_est / R,\n    FDP_ucb = V_ucb / R\n  )\n}\n\ninternal.comparison(adj_p, 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    R V_est V_ucb   FDP_est   FDP_ucb\n1 269    74    90 0.2750929 0.3345725\n```\n:::\n:::\n\n\nIn other words, at 95% confidence, at most 90 of the 269 are noise, meaning 179 of them have actual nonzero effect --- but of course whether the effect is big enough to be meaningful is not up to me.\n\nWould these numbers look better if we use a more strigent threshold? It just so happens that our paper also has a method for it!\n\nWe will not be using all *p*-values this time, but for the used ones, they will fall into one of these cells in this table:\n\n|                               |       Null |   Non-null |      Total |\n|-------------------------------|-----------:|-----------:|-----------:|\n| Small ($p \\le \\alpha / 0.05$) | $V_\\alpha$ | $T_\\alpha$ | $R_\\alpha$ |\n| Big ($p > 0.5$)               |        $U$ |        $W$ |        $B$ |\n| Total                         |      $N_0$ |          * |        $N$ |\n\nThis time the inequality is more involved, from Lemma 1 in the paper (Did we make a typo in there?):\n\n$$B = U + W \\ge_{\\text{st}} \\text{Binomial}\\left(N_0, \\frac{\\alpha / 0.05}{\\alpha / 0.05 + 0.5}\\right) + W \\ge_{\\text{st}} \\text{Binomial}(N - T_\\alpha, \\beta).$$\n\nBut to use it, it is the same: we reject large values of $V_\\alpha$ on the basis of small values of $B$, and get a point estimate for this on the side.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show the code\"}\nexternal.comparison <- function(pval, lambda, alpha) {\n  alpha <- alpha / 0.05\n  stopifnot(lambda > alpha)\n  beta <- (1 - lambda) / (1 - lambda + alpha)\n  R_alpha <- sum(pval < alpha)\n  B <- sum(pval > lambda)\n  V_alpha_est <- (1 - beta) / beta * B\n  N <- R_alpha + B\n  Qs <- 0:N\n  Q <- Qs[\n    max(which(pbinom(B, size = Qs, prob = beta) > 0.05))\n  ]\n  V_alpha_ucb <- Q - B\n  data.frame(\n    alpha = alpha * 0.05,\n    R = R_alpha,\n    V_est = V_alpha_est,\n    V_ucb = V_alpha_ucb,\n    FDP_est = V_alpha_est / R_alpha,\n    FDP_ucb = V_alpha_ucb / R_alpha\n  )\n}\n\nrbind(\n  external.comparison(adj_p, 0.5, 0.01),\n  external.comparison(adj_p, 0.5, 0.005),\n  external.comparison(adj_p, 0.5, 0.001)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  alpha   R V_est V_ucb    FDP_est    FDP_ucb\n1 0.010 195 14.80    23 0.07589744 0.11794872\n2 0.005 174  7.40    13 0.04252874 0.07471264\n3 0.001 127  1.48     4 0.01165354 0.03149606\n```\n:::\n:::",
    "supporting": [
      "nudge-meta-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}