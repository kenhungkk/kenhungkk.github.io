
@article{almohamad2022simultaneous,
  title = {Simultaneous {{Confidence Intervals}} for {{Ranks With Application}} to {{Ranking Institutions}}},
  author = {Al Mohamad, Diaa and Goeman, Jelle J. and {van Zwet}, Erik W.},
  year = {2022},
  month = mar,
  journal = {Biometric Methodology},
  volume = {78},
  number = {1},
  pages = {238--247},
  doi = {10.1111/biom.13419},
  abstract = {When a ranking of institutions such as medical centers or universities is based on an indicator provided with a standard error, confidence intervals should be calculated to assess the quality of these ranks. We consider the problem of constructing simultaneous confidence intervals for the ranks of means based on an observed sample. For this aim, the only available method from the literature uses Monte-Carlo simulations and is highly anticonservative especially when the means are close to each other or have ties. We present a novel method based on Tukey's honest significant difference test (HSD). Our new method is on the contrary conservative when there are no ties. By properly rescaling these two methods to the nominal confidence level, they surprisingly perform very similarly. The Monte-Carlo method is however unscalable when the number of institutions is large than 30 to 50 and stays thus anticonservative. We provide extensive simulations to support our claims and the two methods are compared in terms of their simultaneous coverage and their efficiency. We provide a data analysis for 64 hospitals in the Netherlands and compare both methods. Software for our new methods is available online in package ICRanks downloadable from CRAN. Supplementary materials include supplementary R code for the simulations and proofs of the propositions presented in this paper.},
  langid = {english},
  keywords = {Statistics - Methodology},
  file = {/Users/Kenneth/Zotero/storage/4NKKMYA3/Al Mohamad et al. - 2022 - Simultaneous Confidence Intervals for Ranks With A.pdf}
}

@article{clauset2009powerlaw,
  title = {Power-{{Law Distributions}} in {{Empirical Data}}},
  author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, Mark E. J.},
  year = {2009},
  journal = {SIAM Review},
  volume = {51},
  number = {4},
  pages = {661--703},
  doi = {10.1137/070710111},
  abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution\textemdash the part of the distribution representing large but rare events\textemdash and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov\textendash Smirnov (KS) statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data, while in others the power law is ruled out.},
  langid = {english},
  keywords = {heavy-tailed distributions,likelihood ratio test,maximum likelihood,model selection,Pareto,power-law distributions,Zipf},
  file = {/Users/Kenneth/Zotero/storage/CGXEYUQN/Clauset et al. - 2009 - Power-Law Distributions in Empirical Data.pdf}
}

@article{ellis2020gaining,
  title = {Gaining Power in Multiple Testing of Interval Hypotheses via Conditionalization},
  author = {Ellis, Jules L. and Pecanka, Jakub and Goeman, Jelle J.},
  year = {2020},
  month = apr,
  journal = {Biostatistics},
  volume = {21},
  number = {2},
  pages = {e65-e79},
  abstract = {In this paper we introduce a novel procedure for improving multiple testing procedures (MTPs) under scenarios when the null hypothesis p-values tend to be stochastically larger than standard uniform (referred to as 'inflated'). An important class of problems for which this occurs are tests of interval hypotheses. The new procedure starts with a set of p-values and discards those with values above a certain pre-selected threshold while the rest are corrected (scaled-up) by the value of the threshold. Subsequently, a chosen family-wise error rate (FWER) or false discovery rate (FDR) MTP is applied to the set of corrected p-values only. We prove the general validity of this procedure under independence of p-values, and for the special case of the Bonferroni method we formulate several sufficient conditions for the control of the FWER. It is demonstrated that this 'filtering' of p-values can yield considerable gains of power under scenarios with inflated null hypotheses p-values.},
  file = {/Users/Kenneth/Zotero/storage/CLCWU39S/Ellis et al. - 2020 - Gaining power in multiple testing of interval hypo.pdf}
}

@book{gelman2013bayesian,
  title = {Bayesian {{Data Analysis}}},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  year = {2013},
  abstract = {This electronic edition is for non-commercial purposes only. This electronic edition is for non-commercial purposes only.},
  isbn = {978-1-4398-4095-5},
  file = {/Users/Kenneth/Zotero/storage/9E6J9Q9D/Gelman et al. - 2013 - Bayesian Data Analysis.pdf}
}

@misc{guo2020generalized,
  title = {The {{Generalized Oaxaca-Blinder Estimator}}},
  author = {Guo, Kevin and Basse, Guillaume},
  year = {2020},
  month = apr,
  eprint = {2004.11615},
  eprinttype = {arxiv},
  doi = {10.1080/01621459.2021.1941053},
  abstract = {After performing a randomized experiment, researchers often use ordinary-least squares (OLS) regression to adjust for baseline covariates when estimating the average treatment effect. It is widely known that the resulting confidence interval is valid even if the linear model is misspecified. In this paper, we generalize that conclusion to covariate adjustment with nonlinear models. We introduce an intuitive way to use any ``simple'' nonlinear model to construct a covariate-adjusted confidence interval for the average treatment effect. The confidence interval derives its validity from randomization alone, and when nonlinear models fit the data better than linear models, it is narrower than the usual interval from OLS adjustment.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/Kenneth/Zotero/storage/7DDC7NPE/Guo and Basse - 2020 - The Generalized Oaxaca-Blinder Estimator.pdf}
}

@article{hung2020statistical,
  title = {Statistical {{Methods}} for {{Replicability Assessment}}},
  author = {Hung, Kenneth and Fithian, William},
  year = {2020},
  journal = {The Annals of Applied Statistics},
  volume = {14},
  number = {3},
  pages = {1063--1087},
  doi = {10.1214/20-AOAS1336},
  abstract = {Large-scale replication studies like the Reproducibility Project: Psychology (RP:P) provide invaluable systematic data on scientific replicability, but most analyses and interpretations of the data fail to agree on the definition of ``replicability'' and disentangle the inexorable consequences of known selection bias from competing explanations. We discuss three concrete defini- tions of replicability based on: (1) whether published findings about the signs of effects are mostly correct, (2) how effective replication studies are in reproducing whatever true effect size was present in the original experiment and (3) whether true effect sizes tend to diminish in replication. We apply techniques from multiple testing and postselection inference to develop new methods that answer these questions while explicitly accounting for selection bias. Our analyses suggest that the RP:P dataset is largely consistent with publication bias due to selection of significant effects. The methods in this paper make no distributional assumptions about the true effect sizes. 1.},
  copyright = {All rights reserved},
  keywords = {Meta-analysis,Multiple testing,Post-selection inference,Publication bias,Replicability},
  file = {/Users/Kenneth/Zotero/storage/VGQXM5CB/Hung and Fithian - 2020 - Statistical Methods for Assessing Replicability.pdf}
}

@article{hyman2010diagnosis,
  title = {The {{Diagnosis}} of {{Mental Disorders}}: {{The Problem}} of {{Reification}}},
  shorttitle = {The {{Diagnosis}} of {{Mental Disorders}}},
  author = {Hyman, Steven E.},
  year = {2010},
  month = mar,
  journal = {Annual Review of Clinical Psychology},
  volume = {6},
  number = {1},
  pages = {155--179},
  issn = {1548-5943, 1548-5951},
  doi = {10.1146/annurev.clinpsy.3.022806.091532},
  abstract = {A pressing need for interrater reliability in the diagnosis of mental disorders emerged during the mid-twentieth century, prompted in part by the development of diverse new treatments. The Diagnostic and Statistical Manual of Mental Disorders (DSM), third edition answered this need by introducing operationalized diagnostic criteria that were field-tested for interrater reliability. Unfortunately, the focus on reliability came at a time when the scientific understanding of mental disorders was embryonic and could not yield valid disease definitions. Based on accreting problems with the current DSM-fourth edition (DSM-IV) classification, it is apparent that validity will not be achieved simply by refining criteria for existing disorders or by the addition of new disorders. Yet DSM-IV diagnostic criteria dominate thinking about mental disorders in clinical practice, research, treatment development, and law. As a result, the modern DSM system, intended to create a shared language, also creates epistemic blinders that impede progress toward valid diagnoses. Insights that are beginning to emerge from psychology, neuroscience, and genetics suggest possible strategies for moving forward.},
  langid = {english},
  file = {/Users/Kenneth/Zotero/storage/X47NSI8Y/Hyman - 2010 - The Diagnosis of Mental Disorders The Problem of .pdf}
}

@misc{johari2019always,
  title = {Always {{Valid Inference}}: {{Bringing Sequential Analysis}} to {{A}}/{{B Testing}}},
  shorttitle = {Always {{Valid Inference}}},
  author = {Johari, Ramesh and Pekelis, Leo and Walsh, David J.},
  year = {2019},
  month = jul,
  number = {arXiv:1512.04922},
  eprint = {1512.04922},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.04922},
  abstract = {A/B tests are typically analyzed via frequentist p-values and confidence intervals; but these inferences are wholly unreliable if users endogenously choose samples sizes by *continuously monitoring* their tests. We define *always valid* p-values and confidence intervals that let users try to take advantage of data as fast as it becomes available, providing valid statistical inference whenever they make their decision. Always valid inference can be interpreted as a natural interface for a sequential hypothesis test, which empowers users to implement a modified test tailored to them. In particular, we show in an appropriate sense that the measures we develop tradeoff sample size and power efficiently, despite a lack of prior knowledge of the user's relative preference between these two goals. We also use always valid p-values to obtain multiple hypothesis testing control in the sequential context. Our methodology has been implemented in a large scale commercial A/B testing platform to analyze hundreds of thousands of experiments to date.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Methodology},
  file = {/Users/Kenneth/Zotero/storage/C3LXJ7XW/Johari et al. - 2019 - Always Valid Inference Bringing Sequential Analys.pdf}
}

@misc{kloumann2020optimizable,
  title = {An Optimizable Scalar Objective Value Cannot Be Objective and Should Not Be the Sole Objective},
  author = {Kloumann, Isabel and Tygert, Mark},
  year = {2020},
  month = jun,
  number = {arXiv:2006.02577},
  eprint = {2006.02577},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, math},
  publisher = {{arXiv}},
  abstract = {This paper concerns the ethics and morality of algorithms and computational systems, and has been circulating internally at Facebook for the past couple years. The paper reviews many Nobel laureates' work, as well as the work of other prominent scientists such as Richard Dawkins, Andrei Kolmogorov, Vilfredo Pareto, and John von Neumann. The paper draws conclusions based on such works, as summarized in the title. The paper argues that the standard approach to modern machine learning and artificial intelligence is bound to be biased and unfair, and that longstanding traditions in the professions of law, justice, politics, and medicine should help.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {/Users/Kenneth/Zotero/storage/BPT49ZN8/Kloumann and Tygert - 2020 - An optimizable scalar objective value cannot be ob.pdf}
}

@article{lee2016exact,
  title = {Exact Post-Selection Inference, with Application to the Lasso},
  author = {Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor, Jonathan E.},
  year = {2016},
  month = jan,
  journal = {The Annals of Statistics},
  volume = {44},
  number = {3},
  pages = {907--927},
  abstract = {Abstract We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the ...},
  keywords = {Confidence interval,Hypothesis test,Lasso,Model selection},
  file = {/Users/Kenneth/Zotero/storage/39UEL8PT/Lee et al. - 2016 - Exact post-selection inference, with application t.pdf}
}

@misc{stark2022pay,
  title = {Pay {{No Attention}} to the {{Model Behind}} the {{Curtain}}},
  author = {Stark, Philip B.},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2208.11230},
  abstract = {Many widely used models amount to an elaborate means of making up numbers--but once a number has been produced, it tends to be taken seriously and its source (the model) is rarely examined carefully. Many widely used models have little connection to the real-world phenomena they purport to explain. Common steps in modeling to support policy decisions, such as putting disparate things on the same scale, may conflict with reality. Not all costs and benefits can be put on the same scale, not all uncertainties can be expressed as probabilities, and not all model parameters measure what they purport to measure. These ideas are illustrated with examples from seismology, wind-turbine bird deaths, soccer penalty cards, gender bias in academia, and climate policy.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {Applications (stat.AP),FOS: Computer and information sciences,Methodology (stat.ME)},
  file = {/Users/Kenneth/Zotero/storage/SL6HPU3V/Stark - 2022 - Pay No Attention to the Model Behind the Curtain.pdf}
}

@article{taylor2017post,
  title = {Post-selection Inference for {$\mathscr{l}$}1-penalized Likelihood Models},
  author = {Taylor, Jonathan E. and Tibshirani, Ryan J.},
  year = {2017},
  month = mar,
  journal = {The Canadian Journal of Statistics},
  volume = {19},
  number = {4},
  pages = {1212},
  abstract = {enfrWe present a new method for post-selection inference for {$\mathscr{l}$}1 (lasso)'penalized likelihood models, including generalized regression models. Our approach generalizes the post-selection framework presented...},
  keywords = {Model selection,Selective inference},
  file = {/Users/Kenneth/Zotero/storage/XH62R3UQ/Taylor and Tibshirani - 2017 - Post‐selection inference for ℓ1‐penalized likeliho.pdf}
}

@article{zhao2018multiple,
  title = {Multiple Testing When Many P-Values Are Uniformly Conservative, with Application to Testing Qualitative Interaction in Educational Interventions},
  author = {Zhao, Qingyuan and Small, Dylan S. and Su, Weijie J.},
  year = {2018},
  month = jul,
  journal = {Journal of the American Statistical Association},
  keywords = {Global null,Meta-analysis,Multisite study,Selective inference,Treatment effect heterogeneity,Uniform conditional stochastic order},
  file = {/Users/Kenneth/Zotero/storage/AXSX587X/Zhao et al. - 2018 - Multiple testing when many p-values are uniformly .pdf}
}
