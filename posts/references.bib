
@article{ellis_gaining_2020,
	title = {Gaining power in multiple testing of interval hypotheses via conditionalization},
	volume = {21},
	abstract = {In this paper we introduce a novel procedure for improving multiple testing procedures (MTPs) under scenarios when the null hypothesis p-values tend to be stochastically larger than standard uniform (referred to as 'inflated'). An important class of problems for which this occurs are tests of interval hypotheses. The new procedure starts with a set of p-values and discards those with values above a certain pre-selected threshold while the rest are corrected (scaled-up) by the value of the threshold. Subsequently, a chosen family-wise error rate (FWER) or false discovery rate (FDR) MTP is applied to the set of corrected p-values only. We prove the general validity of this procedure under independence of p-values, and for the special case of the Bonferroni method we formulate several sufficient conditions for the control of the FWER. It is demonstrated that this 'filtering' of p-values can yield considerable gains of power under scenarios with inflated null hypotheses p-values.},
	number = {2},
	journal = {Biostatistics},
	author = {Ellis, Jules L. and Pecanka, Jakub and Goeman, Jelle J.},
	month = apr,
	year = {2020},
	pages = {e65--e79},
	file = {Ellis et al. - 2020 - Gaining power in multiple testing of interval hypo.pdf:/Users/Kenneth/Zotero/storage/CLCWU39S/Ellis et al. - 2020 - Gaining power in multiple testing of interval hypo.pdf:application/pdf},
}

@article{zhao_multiple_2018,
	title = {Multiple testing when many p-values are uniformly conservative, with application to testing qualitative interaction in educational interventions},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2018.1497499},
	journal = {Journal of the American Statistical Association},
	author = {Zhao, Qingyuan and Small, Dylan S. and Su, Weijie J.},
	month = jul,
	year = {2018},
	keywords = {Global null, Meta-analysis, Multisite study, Selective inference, Treatment effect heterogeneity, Uniform conditional stochastic order},
	file = {Zhao et al. - 2018 - Multiple testing when many p-values are uniformly .pdf:/Users/Kenneth/Zotero/storage/AXSX587X/Zhao et al. - 2018 - Multiple testing when many p-values are uniformly .pdf:application/pdf},
}

@article{taylor_postselection_2017,
	title = {Post‐selection inference for ℓ1‐penalized likelihood models},
	volume = {19},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/cjs.11313/full},
	abstract = {enfrWe present a new method for post‐selection inference for ℓ1 (lasso)'penalized likelihood models, including generalized regression models. Our approach generalizes the post‐selection framework presented...},
	number = {4},
	journal = {The Canadian Journal of Statistics},
	author = {Taylor, Jonathan E. and Tibshirani, Ryan J.},
	month = mar,
	year = {2017},
	keywords = {Model selection, Selective inference},
	pages = {1212},
	file = {Taylor and Tibshirani - 2017 - Post‐selection inference for ℓ1‐penalized likeliho.pdf:/Users/Kenneth/Zotero/storage/XH62R3UQ/Taylor and Tibshirani - 2017 - Post‐selection inference for ℓ1‐penalized likeliho.pdf:application/pdf},
}

@article{lee_exact_2016,
	title = {Exact post-selection inference, with application to the lasso},
	volume = {44},
	url = {http://projecteuclid.org/euclid.aos/1460381681},
	abstract = {Abstract We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the ...},
	number = {3},
	journal = {The Annals of Statistics},
	author = {Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor, Jonathan E.},
	month = jan,
	year = {2016},
	keywords = {Model selection, Hypothesis test, Lasso, Confidence interval},
	pages = {907--927},
	file = {Lee et al. - 2016 - Exact post-selection inference, with application t.pdf:/Users/Kenneth/Zotero/storage/39UEL8PT/Lee et al. - 2016 - Exact post-selection inference, with application t.pdf:application/pdf},
}

@book{gelman_bayesian_2013,
	title = {Bayesian {Data} {Analysis}},
	isbn = {978-1-4398-4095-5},
	abstract = {This electronic edition is for non-commercial purposes only. This electronic edition is for non-commercial purposes only.},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	year = {2013},
	file = {Gelman et al. - 2013 - Bayesian Data Analysis.pdf:/Users/Kenneth/Zotero/storage/9E6J9Q9D/Gelman et al. - 2013 - Bayesian Data Analysis.pdf:application/pdf},
}

@article{clauset_power-law_2009,
	title = {Power-{Law} {Distributions} in {Empirical} {Data}},
	volume = {51},
	doi = {10.1137/070710111},
	abstract = {Power-law distributions occur in many situations of scientiﬁc interest and have signiﬁcant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large ﬂuctuations that occur in the tail of the distribution—the part of the distribution representing large but rare events—and by the diﬃculty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares ﬁtting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood ﬁtting methods with goodness-of-ﬁt tests based on the Kolmogorov–Smirnov (KS) statistic and likelihood ratios. We evaluate the eﬀectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of diﬀerent disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we ﬁnd these conjectures to be consistent with the data, while in others the power law is ruled out.},
	language = {en},
	number = {4},
	journal = {SIAM Review},
	author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, Mark E. J.},
	year = {2009},
	keywords = {power-law distributions, Pareto, Zipf, maximum likelihood, heavy-tailed distributions, likelihood ratio test, model selection},
	pages = {661--703},
	file = {Clauset et al. - 2009 - Power-Law Distributions in Empirical Data.pdf:/Users/Kenneth/Zotero/storage/CGXEYUQN/Clauset et al. - 2009 - Power-Law Distributions in Empirical Data.pdf:application/pdf},
}

@misc{guo_generalized_2020,
	title = {The {Generalized} {Oaxaca}-{Blinder} {Estimator}},
	url = {http://arxiv.org/abs/2004.11615},
	abstract = {After performing a randomized experiment, researchers often use ordinary-least squares (OLS) regression to adjust for baseline covariates when estimating the average treatment eﬀect. It is widely known that the resulting conﬁdence interval is valid even if the linear model is misspeciﬁed. In this paper, we generalize that conclusion to covariate adjustment with nonlinear models. We introduce an intuitive way to use any “simple” nonlinear model to construct a covariate-adjusted conﬁdence interval for the average treatment eﬀect. The conﬁdence interval derives its validity from randomization alone, and when nonlinear models ﬁt the data better than linear models, it is narrower than the usual interval from OLS adjustment.},
	language = {en},
	urldate = {2022-03-31},
	author = {Guo, Kevin and Basse, Guillaume},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.11615},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
	file = {Guo and Basse - 2020 - The Generalized Oaxaca-Blinder Estimator.pdf:/Users/Kenneth/Zotero/storage/7DDC7NPE/Guo and Basse - 2020 - The Generalized Oaxaca-Blinder Estimator.pdf:application/pdf},
}

@misc{johari_always_2019,
	title = {Always {Valid} {Inference}: {Bringing} {Sequential} {Analysis} to {A}/{B} {Testing}},
	shorttitle = {Always {Valid} {Inference}},
	url = {http://arxiv.org/abs/1512.04922},
	doi = {10.48550/arXiv.1512.04922},
	abstract = {A/B tests are typically analyzed via frequentist p-values and confidence intervals; but these inferences are wholly unreliable if users endogenously choose samples sizes by *continuously monitoring* their tests. We define *always valid* p-values and confidence intervals that let users try to take advantage of data as fast as it becomes available, providing valid statistical inference whenever they make their decision. Always valid inference can be interpreted as a natural interface for a sequential hypothesis test, which empowers users to implement a modified test tailored to them. In particular, we show in an appropriate sense that the measures we develop tradeoff sample size and power efficiently, despite a lack of prior knowledge of the user's relative preference between these two goals. We also use always valid p-values to obtain multiple hypothesis testing control in the sequential context. Our methodology has been implemented in a large scale commercial A/B testing platform to analyze hundreds of thousands of experiments to date.},
	language = {en},
	urldate = {2022-09-02},
	publisher = {arXiv},
	author = {Johari, Ramesh and Pekelis, Leo and Walsh, David J.},
	month = jul,
	year = {2019},
	note = {arXiv:1512.04922 [math, stat]},
	keywords = {Statistics - Applications, Statistics - Methodology, Mathematics - Statistics Theory},
	file = {Johari et al. - 2019 - Always Valid Inference Bringing Sequential Analys.pdf:/Users/Kenneth/Zotero/storage/C3LXJ7XW/Johari et al. - 2019 - Always Valid Inference Bringing Sequential Analys.pdf:application/pdf},
}

@article{al_mohamad_simultaneous_2022,
	title = {Simultaneous {Confidence} {Intervals} for {Ranks} {With} {Application} to {Ranking} {Institutions}},
	volume = {78},
	doi = {10.1111/biom.13419},
	abstract = {When a ranking of institutions such as medical centers or universities is based on an indicator provided with a standard error, conﬁdence intervals should be calculated to assess the quality of these ranks. We consider the problem of constructing simultaneous conﬁdence intervals for the ranks of means based on an observed sample. For this aim, the only available method from the literature uses Monte-Carlo simulations and is highly anticonservative especially when the means are close to each other or have ties. We present a novel method based on Tukey’s honest signiﬁcant diﬀerence test (HSD). Our new method is on the contrary conservative when there are no ties. By properly rescaling these two methods to the nominal conﬁdence level, they surprisingly perform very similarly. The Monte-Carlo method is however unscalable when the number of institutions is large than 30 to 50 and stays thus anticonservative. We provide extensive simulations to support our claims and the two methods are compared in terms of their simultaneous coverage and their eﬃciency. We provide a data analysis for 64 hospitals in the Netherlands and compare both methods. Software for our new methods is available online in package ICRanks downloadable from CRAN. Supplementary materials include supplementary R code for the simulations and proofs of the propositions presented in this paper.},
	language = {en},
	number = {1},
	urldate = {2022-09-02},
	journal = {Biometric Methodology},
	author = {Al Mohamad, Diaa and Goeman, Jelle J. and van Zwet, Erik W.},
	month = mar,
	year = {2022},
	keywords = {Statistics - Methodology},
	pages = {238--247},
	file = {Al Mohamad et al. - 2022 - Simultaneous Confidence Intervals for Ranks With A.pdf:/Users/Kenneth/Zotero/storage/4NKKMYA3/Al Mohamad et al. - 2022 - Simultaneous Confidence Intervals for Ranks With A.pdf:application/pdf},
}
