[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Spring 2019: GSI for Math 110 Linear Algebra with Olga Holtz\nFall 2018: GSI for Math 55 Discrete Mathematics with Mark Haiman\nSpring 2018: GSI for Math 53 Multivariable Calculus with Zvezdelina Stankova\nFall 2017: GSI for Math 1a Calculus with Vivek Shende\nFall 2016: GSI for Stat 210a Theoretical Statistics with Will Fithian\nSpring 2016: GSI for Math 1b Calculus with Alexander Paulin\nFall 2015: GSI for Math 55 Discrete Mathematics with David Li-Brand\nSummer 2015: Instructor for Math 54 Linear Algebra and Differential Equations with David Li-Brand\nSpring 2015: GSI for Math 1b Calculus with Alexander Coward\nFall 2014: GSI for Math 1a Calculus with Xinyi Yuan\n\nCaltech\n\nSpring 2014: TA for CS 38 Introduction to Algorithms with Chris Umans\nWinter 2014: TA for CS 21 Decidability and Tractability with Chris Umans"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Critical groups of strongly regular graphs and their generalizations. Kenneth Hung and Chi Ho Yuen, Innovations in Incidence Geometry: Algebraic, Topological and Combinatorial, 2022, to appear.\nStatistical methods for replicability assessment. Kenneth Hung and William Fithian, Annals of Applied Statistics, 2020. (pdf, aoas, git)\nRank verification for exponential families. Kenneth Hung and William Fithian, Annals of Statistics, 2019. (pdf, aos, git)\n\nPresentations\n\n\n\nLarge-scale metric defense (poster at CODE 2021)\n\n\n\n\nStatistical methods for replicability assessment (invited talk at JSM 2021, invited talk at ISSI)\n\n\n\n\nRank verification for exponential families (talk at MCP 2017, poster at WHOA-PSI 2017)"
  },
  {
    "objectID": "posts/p-value-screening.html",
    "href": "posts/p-value-screening.html",
    "title": "p-value screening",
    "section": "",
    "text": "Now a good lower confidence bound would one that is larger. We can consider the test dual to the confidence bound: a test with higher power generally should mean a better confidence bound. The dual test will test the null hypothesis\n\\[H_0^\\tau: \\max_i \\mu_i \\le \\tau,\\]\nwhich is really just the intersection of \\(H_{0, i}^\\tau: \\mu_i \\le \\tau\\). We are now in the realm of multiple testing!\nA basic idea to test \\(H_0^\\tau\\) is of course through Bonferroni correction. However, as we test \\(H_0^\\tau\\) for \\(\\tau\\) that is larger than many of the observations \\(X_i\\)’s, few of these observations will provide evidence against the null but they count towards the multiplicity nonetheless. This is where we can aim to do better.\nZhao, Small and Su (2018) suggested to use a \\(\\lambda\\) to screen out the p-values under \\(\\lambda\\), divide the remaining p-values by \\(\\lambda\\). (This idea was also rediscovered by Ellis, Pecanka, Goeman (2017).) We have also rediscovered the same idea, and all three groups stated very similar conditions for the p-values, from “uniformly conservative” to “supreuniform”. This \\(\\lambda\\) can also be taken as a stopping time: \\(\\lambda\\) can go from 0 to 1, revealing any p-value greater than \\(\\lambda\\) and stopping based only on this information. It can be shown that this still controls the type I error rate, through a martingale argument not unlike the proof of Benjamini–Hochberg procedure.\nBut Zhao, Small and Su (2018) took an extra step in proposing a beautiful method for selecting this \\(\\lambda\\): Note that the “saving” ones gets from performing the p-value screening is\n\\[\\frac{F(\\lambda)}{\\lambda},\\]\nwhere \\(F(\\lambda)\\) is the averaged distribution of the p-values. The smaller the ratio above, the better the test will perform. Taking the derivatives give\n\\[\\frac{d}{\\lambda} \\frac{F(\\lambda)}{\\lambda} = \\frac{F'(\\lambda) \\lambda - F(\\lambda)}{\\lambda^2},\\]\nand a good choice of stopping is when there is no strong evidence that \\(F'(\\lambda) \\lambda - F(\\lambda) > 0\\). In general this should eventually happen, either when \\(\\lambda\\) gets uncomfortably close to 0, or when most of the true nulls have been removed.\nThis approach is great for testing, but comes with two caveats when it comes to finding lower confidence bounds: - We do not know if this test is monotone: it is possible that the test rejects at \\(\\tau\\) yet accepts at some \\(\\tau' < \\tau\\). This may happen if the multiplicity correction is substantially greater for testing \\(H_0^{\\tau'}\\) than at \\(H_0^{\\tau}\\). - If we are not using the Zhao, Small and Su (2018) method for picking \\(\\lambda\\) and leaving this to an analyst’s discretion, the analyst may not be principled enough. They may be using a \\(\\lambda\\), while wishing that they have used stopped earlier.\nThe later point raises a question, similar to that in Johari, Pekelis and Walsh (2016) or “spotting time” as suggested by Aaditya Ramdas: Is it possible to allow an analyst stop anytime they want? If so, allowing an analyst to go backwards should come with a price. How do we make an appropriate correction?\nChoosing a screening threshold at \\(\\lambda\\) is essentially the same as using\n\\[n p_{(1)} \\frac{F_n(\\lambda)}{\\lambda}\\]\nas the test statistic and reject for small values. Here \\(p_{(1)}\\) is the smallest p-value and \\(F_n\\) is the empirical CDF of the p-values. What we want to do is essentially using the test statistic\n\\[n p_{(1)} \\min_{\\lambda \\in \\Lambda_0} \\frac{F_n(\\lambda)}{\\lambda}\\]\nfor some subset \\(\\Lambda_0 \\subset (0, 1]\\), and rejecting for small values. This is equivalent to allow an analyst to search through \\(\\Lambda_0\\) to cherry pick the best looking \\(\\lambda\\). Some possible options of \\(\\Lambda_0\\) are \\([\\lambda_0, 1]\\) for some prespecified \\(\\lambda_0\\) or \\(\\Lambda_0 = [p_{(k)}, 1]\\) where \\(p_{(k)}\\) is the \\(k\\)-th smallest p-value.\nThe first one, while maybe more practical, is less interesting as it probably is not too different from using a fixed \\(\\lambda_0\\). We will turn to the second option here. We will analyse it by finding a “stochastic lower bound” for this statistic. We have\n\\[\\begin{eqnarray}\nn p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda)}{\\lambda} &=& n p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F(\\lambda)}{\\lambda} \\frac{F_n(\\lambda)}{F(\\lambda)} \\\\\n&\\stackrel{\\text{st}}{\\ge}& n p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F(\\lambda)}{\\lambda} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda)}{F(\\lambda)} \\\\\n&=& n p_{(1)} \\frac{F(p_{(1)})}{p_{(1)}} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda)}{F(\\lambda)} \\\\\n&=& n F(p_{(1)}) \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda)}{F(\\lambda)}.\n\\end{eqnarray}\\]\nOf course, \\(n F(p_{(1)})\\) is stochastically larger than \\(U[0, 1]\\). So it suffices to control \\(\\min_{\\lambda \\ge p_{(k)}} F_n(\\lambda) / F(\\lambda)\\), or equivalently, make sure \\(\\max_{\\lambda \\ge p_{(k)}} F(\\lambda) / F_n(\\lambda)\\) is not too big. We consider the least favorable distribution, where all p-values are uniformly distributed, i.e. \\(F(p) = p\\).\nThis quantity of interest may look like something from empirical process, but we can focus on its value at one of the p-values. So it is good enough to look at\n\\[F(p_{(t)}) / F_n(p_{(t)}) = n p_{(t)} / t,\\]\nwhich happens to be a martingale under the filtration \\(\\mathcal{F}_t = \\sigma(\\{p_{(t)}, \\ldots, p_{(n)}\\})\\) as \\(t\\) decreases from \\(n\\) to \\(k\\). The expectation of each term in \\(n / (n+1)\\), so we can use Doob’s martingale inequality on the submartingale\n\\[\\left(\\frac{p_{(t)}}{t} - \\frac{n}{n+1}\\right)^2\\]\nto get some concentration, giving\n\\[\\begin{eqnarray}\n\\mathbb{P}\\left[\\max_{k \\le t \\le n} \\left(\\frac{n p_{(t)}}{t} - \\frac{n}{n+1}\\right)^2 > C\\right] &\\le& \\frac{\\mathrm{var}[n p_{(k)} / k]}{C} \\\\\n&=& \\frac{1}{C} \\cdot \\frac{n^2 (n+1-k)}{k (n+1)^2 (n+2)}.\n\\end{eqnarray}\\]\nThere probably is a better bound, but for now let’s stick with this. Under the null we have\n\\[\\begin{eqnarray}\n\\mathbb{P}\\left[\\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda) + \\frac{1}{n}}{\\lambda} < \\frac{1}{C}\\right] &\\le& \\mathbb{P}\\left[\\max_{k \\le t \\le n} \\frac{p_{(k)}}{k} < C\\right] \\\\\n&\\le& \\frac{1}{\\left(C - \\frac{n}{n+1}\\right)^2} \\cdot \\frac{n^2 (n+1-k)}{k (n+1)^2 (n+2)}\n\\end{eqnarray}\\]\nfor any \\(C > \\frac{n+1}{n}\\). With this bound, we have that\n\\[C n p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda) + \\frac{1}{n}}{\\lambda} + \\frac{1}{\\left(C - \\frac{n}{n+1}\\right)^2} \\cdot \\frac{n^2 (n+1-k)}{C k (n+1)^2 (n+2)}\\]\nis stochastically larger than \\(U[0, 1]\\) for any \\(C > \\frac{n}{n+1}\\). Optimizing over \\(C\\) gives that\n\\[\\frac{n^2}{n+1} p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda) + \\frac{1}{n}}{\\lambda} + \\frac{3}{2^{2/3}} \\left(np_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda) + \\frac{1}{n}}{\\lambda}\\right)^{2/3} \\left(\\frac{n^2 (n+1-k)}{k (n+1)^2 (n+2)}\\right)^{1/3}\\]\nis also stochastically larger than \\(U[0, 1]\\), so we can use this as our p-value.\nIn a pessimistic case where the analyst shows no restraint, there is nevertheless no reason to choose \\(\\lambda \\le p_{(2)}\\), so the smallest \\(k\\) is 2. Now the question comes: how does this test fare compared to Zhao, Small, Su (2018)?\nWe wrote a short piece of code to test this:\n\nsuppressPackageStartupMessages(library(foreach))\n\nspotting.test <- function(x, k = 2) {\n  n <- length(x)\n  p <- sort(pnorm(x, lower.tail = FALSE))\n  multiplier <- min((k:n) / p[k:n] / n)\n  comb.p <-\n    n^2 / (n + 1) * p[1] * multiplier +\n    3 / 2^(2 / 3) * (n * p[1] * multiplier)^(2 / 3) *\n    (n^2 * (n + 1 - k) / k / (n + 1)^2 / (n + 2))^(1 / 3)\n  min(comb.p, 1)\n}\n\nspotting.power <- function(mu, k = 2) {\n  require(foreach)\n  rej <- foreach(i = 1:10000, .combine = \"c\") %do% {\n    x <- mu + rnorm(length(mu))\n    spotting.test(x, k) < 0.05\n  }\n  mean(rej)\n}\n\nc(\n  spotting.power(rep(0, 100)),\n  spotting.power(c(4, rep(0, 99))),\n  spotting.power(c(4, rep(-1, 99))),\n  spotting.power(c(4, rep(-4, 99))),\n  spotting.power(c(4, rep(-10, 99))),\n  spotting.power(c(rep(1, 20), rep(0, 80))),\n  spotting.power(c(rep(1, 20), rep(-1, 80))),\n  spotting.power(c(rep(1, 20), rep(-4, 80)))\n)\n\n[1] 0.0089 0.5746 0.7095 0.8827 0.8889 0.0428 0.0595 0.1163\n\n\nSince post condition we are really just using Bonferroni test, we will compare to those particular rows in Table 2 in Zhao, Small, Su (2018):\n\n\n\n\n\n\n\n\n\nSetting\n\\(\\tau\\) = 0.5\nAdaptive\nSpotting\n\n\n\n\n1. All null\n5.0\n5.0\n0.7\n\n\n2. 1 strong 99 null\n76.6\n76.7\n57.3\n\n\n3. 1 strong 99 conservative\n85.2\n84.0\n70.9\n\n\n4. 1 strong 99 very conservative\n98.0\n98.7\n88.3\n\n\n5. 1 strong 99 extremely conservative\n97.8\n98.9\n89.0\n\n\n6. 20 weak 80 null\n21.0\n22.5\n4.6\n\n\n7. 20 weak 80 conservative\n28.1\n26.3\n6.0\n\n\n8. 20 weak 80 very conservative\n38.1\n47.3\n11.6\n\n\n\nWelp. This does not work that well. One possible reason is that “Adaptive” does a good job capturing the best cutoff already, spotting needs to account for too much noise and pays an unnecessarily high price. In either case, it is not clear if the spotting test is monotone anyway."
  },
  {
    "objectID": "posts/rnorm-in-python.html",
    "href": "posts/rnorm-in-python.html",
    "title": "Normal distribution in Python",
    "section": "",
    "text": "Working on theoretical statistics, all of my work in my PhD was done in R. But for production code in industry, for the sake of speed and easier maintenance, I have taken up to implement some of my ideas in Python even when the prototyping is done in R. I did not expect to be learning this much by just looking at a normal distribution."
  },
  {
    "objectID": "posts/rnorm-in-python.html#scipy.stats-vs-math",
    "href": "posts/rnorm-in-python.html#scipy.stats-vs-math",
    "title": "Normal distribution in Python",
    "section": "scipy.stats vs math",
    "text": "scipy.stats vs math\nSuppose we want to compute the CDF of a standard normal. An R user like me who is used to pnorm(z) would probably write this\nfrom scipy.stats import norm\n\nnorm.cdf(z)\nBut another way is to use the erf function, given by\n\\[\n\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z e^{-t^2} \\,dt.\n\\]\nWe can then write\n\nfrom math import erf\n\ndef phi(z):\n  return (1.0 + erf(z / sqrt(2.0))) / 2.0\n\nIn fact this is the given example in the Python documentations for the math library.\nFor my purpose, this function needs to be run many times, so speed is definitely important here. We run it for 10000 times:\n\nfrom timeit import default_timer\nfrom scipy.stats import norm\nfrom math import erf\n\ndef phi(z):\n  return (1.0 + erf(z / 1.4142135623730951)) / 2.0\n\nstart = default_timer()\nscipy_output = [norm.cdf(1) for i in range(10000)]\nend = default_timer()\nprint(\"scipy took:\", str(end - start))\n\nscipy took: 0.9739241569999999\n\nstart = default_timer()\nmath_output = [phi(1) for i in range(10000)]\nend = default_timer()\nprint(\"math took:\", str(end - start))\n\nmath took: 0.011614696000000091\n\n\nComputing using erf was a lot faster than using scipy, but that should not come as a surprise. Even in R, a loop without any vectorization is bound to be slow. After all, the strength of scipy is that it works well with numpy arrays. So let’s vectorize it:\n\nimport numpy\n\nstart = default_timer()\nnorm.cdf(numpy.repeat(1, 10000))\n\narray([0.84134475, 0.84134475, 0.84134475, ..., 0.84134475, 0.84134475,\n       0.84134475])\n\nend = default_timer()\nprint(\"scipy took:\", str(end - start))\n\nscipy took: 0.013408950000000086\n\n\nToo bad that in my use case, the CDF of the normal distribution has to be computed in a loop, so I will go with the erf route…"
  },
  {
    "objectID": "posts/rnorm-in-python.html#erf-vs-erfc",
    "href": "posts/rnorm-in-python.html#erf-vs-erfc",
    "title": "Normal distribution in Python",
    "section": "erf vs erfc",
    "text": "erf vs erfc\n…which brings us to another question. How accurate is computing the CDF based on erf? The part where we add erf to 1.0 means that anything that is close to or smaller than the machine epsilon will get erased. What if we do care about the magnitude of the CDF far in the tail of the normal? With the time constraint, we cannot really call norm.logcdf here.\n\n[phi(-z) for z in range(5, 15)]\n\n[2.8665157186802404e-07, 9.865876449133282e-10, 1.2798095916366492e-12, 6.106226635438361e-16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n\nWe can instead use erfc, defined as\n\\[\n\\text{erfc}(z) = 1 - \\text{erf}(z).\n\\]\nNow we have\n\nfrom math import erfc\n\ndef phi(z):\n  return erfc(-z / 1.4142135623730951) / 2.0\n\n[phi(-z) for z in range(5, 15)]\n\n[2.866515718791945e-07, 9.865876450377014e-10, 1.2798125438858348e-12, 6.22096057427182e-16, 1.1285884059538425e-19, 7.619853024160593e-24, 1.9106595744986828e-28, 1.7764821120777016e-33, 6.117164399549921e-39, 7.793536819192798e-45]\n\n\nIt preserved many more digits. It is curious how the Python documentations chose to use this example for erf instead of erfc. To check if this is correct, I run in R:\n\npnorm(-(5:14))\n\n [1] 2.866516e-07 9.865876e-10 1.279813e-12 6.220961e-16 1.128588e-19\n [6] 7.619853e-24 1.910660e-28 1.776482e-33 6.117164e-39 7.793537e-45\n\n\nwhich gave the exactly same numbers. Did R perhaps implement pnorm using erfc as well?"
  },
  {
    "objectID": "posts/moment-conditions.html",
    "href": "posts/moment-conditions.html",
    "title": "Moment conditions heuristics",
    "section": "",
    "text": "\\[\\text{var}(X_i) = \\sigma^2 < \\infty.\\]\nThis classic formulation of Central Limit Theorem is often used in estimating the population mean when we only have a small sample, where we get a point estimate and an estimated standard error, then apply the normal approximation. But how do we know if the theorem “applies”? I’ve usually gotten two answers, or a combination of both: - If you are so worried about the normal approximation, use bootstrap. - The theorem always apply, since you technically have a finite population, so you technically have a finite variance regardless. - (Occasionally) The theorem always apply, and how well the approximation is governed by Berry–Esseen theorem. (I guess this is somewhat helpful, but then I’m stuck figuring out what the third moment is.)\nFor the first answer, my intuition is that when normal approximation is poor, bootstrap (at least standard percentile bootstrap) will be poor too, since the proofs of bootstrap I am familiar with goes through normal distributions too (e.g. van der Vaart, Chapter 23.2.1).\nFor the second answer, I still want to know if the approximation is good. Philosophically, this is similar to doing high-dimensional linear regression — when we have \\(n = 1000\\) and \\(p = 100\\), should we be looking at results on \\(p = O(n)\\) or \\(p = O(1)\\)? The idea here is the same: when we look at our data, we can sometimes tell that the normal approximation is going to fail if the top 10% of your sample accounts adds up to 90% of the sum. Can we have some heuristic that these moment conditions are going to fail?\nMore generally, I want to be able to look “empirically check” for moment conditions that look like\n\\[\\frac{1}{n} \\sum_{i=1}^n |X_i|^\\beta = o(n^\\gamma).\\]\nIn most cases I’ve encountered, \\(\\beta\\) would be an integer, and I am including a \\(\\gamma\\) there as well to include cases like Guo and Basse (2021), Example 1:\n\\[\\frac{1}{n} \\sum_{i=1}^n X_i^4 = o(n).\\]\nBelow are back-of-the-envelope calculations that I’ve been using recently to reason around these conditions. We are going to start with the stable distribution, with tail index \\(\\alpha\\). Recall that if \\(X, Y \\sim \\text{Stable}(\\alpha)\\) independently, then \\(X + Y \\sim 2^{1/\\alpha} \\cdot \\text{Stable}(\\alpha)\\). Now in cases where the generalized Central Limit Theorem works, we will be able to get, with some scaling and some choice of \\(\\alpha\\),\n\\[\\frac{1}{n/2} \\sum_{i=1}^{n/2} |X_i|^\\beta, \\frac{1}{n/2} \\sum_{i=n/2 + 1}^{n} |X_i|^\\beta \\sim \\text{Stable}(\\alpha),\\]\nand combining them will get\n\\[\\frac{1}{n} \\sum_{i=1}^n |X_i|^\\beta \\sim 2^{1/\\alpha - 1} \\cdot \\text{Stable}(\\alpha).\\]\nRepeating this ad infinitum, we have\n\\[\\frac{1}{n} \\sum_{i=1}^n |X_i|^\\beta = O_p((2^{1/\\alpha - 1})^{\\log_2 n}) = O_p(n^{1/\\alpha - 1}).\\]\nSo for example, in the case of Guo and Basse (2021), Example 1, we will need \\(X_i^\\beta\\) to be no more heavy-tailed than \\(\\alpha = 1/2\\). But now, since \\(X \\sim \\text{Stable}(\\alpha)\\) is roughly the boundary where \\(\\mathbb{E}[X^\\alpha]\\) exists, Guo and Basse (2021) should be roughly the same as asking the tail index to be at least 2, pretty much the same as asking that Central Limit Theorem applies.\nI think the heuristics above can be made a bit more rigorous too, probably through \\(\\mathbb{P}[X > n^a] > n^{-b}\\) for some choice of \\(a\\) and \\(b\\), these choices of \\(a\\) and \\(b\\) can perhaps be compared to a generalized Pareto distribution.\nFinally to check this, without getting into the subtleties of Hill estimator or Clauset et al. (2009), we can look at some high quantiles and take the ratio. For example, a rough estimate of the tail index would be \\(1 / \\log_{10}(q_{0.999} / q_{0.99})\\)."
  },
  {
    "objectID": "posts/asymptotic-selective-inference.html",
    "href": "posts/asymptotic-selective-inference.html",
    "title": "L1-penalized likelihood asymptotics",
    "section": "",
    "text": "Most specifically, the set up is as follows: we have a matrix \\(X\\) consisting of \\(n\\) row of covariates, \\(Y\\) a vector consisting of all the responses. We assume the model, given by the log-likelihood below,\n\\[\\sum_{i=1}^n \\ell(\\theta_n; Y_n, X_n).\\]\nHere we will not move into high-dimensional regime and thus assumes \\(\\theta_n\\) to have dimension \\[d\\]. Subsequently, we perform selection based on maximizing\n\\[\\sum_{i=1}^n \\ell(\\theta_n; Y_n, X_n) - \\lambda_n \\|\\theta_n\\|_1,\\]\nwith some tuning parameter \\(\\lambda_n\\).\nAround three weeks ago, Will and I came up with a way to tackle this problem — together with a non-exhaustive, non-optimized list of conditions needed. Unfortunately shortly after Will discovered a paper by Taylor and Tibshirani (2017) that arrived at an almost identical solution. While our result might no longer be groundbreaking, we hope that this post will provide a different perspective from Taylor and Tibshirani (2017), and assist anyone who happens to also be reading Taylor and Tibshirani (2017).\nThe problem has two main hurdles: - approximating the selection event in a reasonable yet theoretically valid manner; - choosing a test statistic with a nice asymptotic distribution.\nWe can make both decisions at once by considering the selection event. In GLM, with a sufficient statistic, the selection event will always be measurable with respect to this sufficient statistic. This measurability requirement results in fuzzy edges if we plot out the selection event based on a non-sufficient statistic.\nWe don’t have this sufficient statistic anymore in a general likelihood setting. Conventionally, both the score at a fixed parameter and the MLE are thought of as ‘asymptotically sufficient’ without a proper definition. Since we are looking into asymptotics anyways, these two statistic seems perfect for our use. A bonus is that their asymptotic distributions are well known.\nFollowing classical asymptotic analysis as explained in van der Vaart (1998), we will assume that \\(\\theta_n = \\theta_0 + \\beta / \\sqrt{n}\\) and thus converges to a \\(\\theta_0\\) that lies in the null hypothesis \\(\\Theta_0\\). Other possible asymptotic regimes includes modifying the lasso minimization problem into a ‘non-centered’ lasso problem\n\\[\\sum_{i=1}^n \\ell(\\theta_n; Y_n, X_n) - \\lambda_n \\|\\theta_n - c_n\\|_1,\\]\nbut as it turns out the asymptotics will work out to the same solution anyways. For the lasso selection to not go trivial (always selecting certain variables, always not selecting certain variables, always making the correct selection), we also need to scale \\(\\lambda_n\\) as \\(\\lambda_n = \\lambda \\sqrt{n}\\).\nIf we take the subgradient of the objective, normalized by \\(1 / \\sqrt{n}\\), with respect to \\(\\theta_n\\), we get something like\n\\[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\nabla \\ell(\\theta_n; Y_n, X_n) - \\lambda s_n,\\]\nwhere \\(s_n\\) is the subgradient of the \\(L_1\\)-norm. This is the crucial step in Lee et al. (2013). For a the same set of variables selected and the same signs assigned, \\(\\lambda s_n\\) is a determined set. So what’s left is to relate the normalized score to the sufficient statistic.\nIn the asymptotic regime, the asymptotic sufficiency of score and the MLE means we can determine all the likelihood ratio, or equivalently, the entire sore function. From here we can approximate the score as a linear function at \\(0\\) as\n\\[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\nabla \\ell(\\theta_n; Y_n, X_n) \\approx \\left[\\frac{1}{n} \\sum_{i=1}^n \\nabla^2 \\ell(0; Y_n, X_n)\\right] \\beta,\\]\nor as a linear function based at the MLE \\(\\hat{\\theta}_n\\) (and hence \\(\\hat{\\beta}\\)),\n\\[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\nabla \\ell(\\theta_n; Y_n, X_n) \\approx \\left[\\frac{1}{n} \\sum_{i=1}^n \\nabla^2 \\ell(\\hat{\\theta}_n; Y_n, X_n)\\right] (\\beta - \\hat{\\beta}).\\]\nWe cannot however approximate this as a linear function at other points, such as the MLE restricted to the null hypothesis \\(\\Theta_0\\), as it reduces the degree of freedom.\nHow do we choose between these two approximation? In finite sample, the ‘data’ might not lie close to \\(0\\), rendering the first approximation ill-motivated. The second one has an appeal that it moves with the data and tends to approximate the score function better locally near the MLE.\nTo be more concrete, we can have a look at this in practice. We generated 1000 samples of 100 points from a logistic model and ran glmnet on each of the 100 samples. The unrestricted MLE is used as the statistic and plotted below. Colors follow the signs and the variables selected.\n\n\nShow the code\nsuppressPackageStartupMessages({\n  library(foreach)\n  library(doParallel)\n  library(glmnet)\n  library(ggplot2)\n  library(dplyr)\n})\n\ntheme_set(theme_minimal())\n\nLogistic <- function(z) {\n  return(exp(z) / (1 + exp(z)))\n}\n\nSignSummary <- function(x) {\n  return(paste(ifelse(x == 0, '0', ifelse(x > 0, '+', '-')), collapse = ''))\n}\n\nHessianLogistic <- function(x, theta) {\n  z <- x %*% theta\n  psi.2nd <- exp(z) / (1 + exp(z))^2\n  return(-t(x) %*% (x * as.vector(psi.2nd)) / nrow(x))\n}\n\nHessianGaussian <- function(x) {\n  return(-t(x) %*% x / nrow(x))\n}\n\nGLMSim <- function(theta, lambda, n, nRep = 1000, family = 'gaussian') {\n  # GLMSim produces a plot of the true selection events by simulation, with\n  # approximation of the selection event based on Taylor expansion of the\n  # log-likelihood at the true theta (dahsed) and the MLE (dotted)\n  #\n  # Args:\n  #   theta: true parameter\n  #   lambda: penalty parameter\n  #   n: sample size\n  #   nRep: number of points to be included\n  #   family: 'binomial' for logistic regression, 'gaussian' for linear\n  #     regression\n  #\n  # Returns:\n  #   A plot of the simulation\n  registerDoParallel(cores = 4)\n  \n  # generates nRep possible sets of n observations\n  x.cov.rt <- rbind(c(1, -0.1), c(-0.1, 1))\n  x <- matrix(rnorm(n * 2), n, 2) %*% x.cov.rt\n  if (family == 'binomial') {\n    pr <- Logistic(x %*% theta)\n    y <- foreach(i = 1:nRep, .combine = cbind) %dopar% {\n      set.seed(i)\n      rbinom(n, size = 1, prob = pr)\n    }\n  } else if (family == 'gaussian') {\n    mu <- x %*% theta\n    y <- foreach(i = 1:nRep, .combine = cbind) %dopar% {\n      set.seed(i)\n      rnorm(n, mean = mu)\n    }\n  } else {\n    stop('family is not binomial or gaussian')\n  }\n  \n  # runs glmnet to compute the selected variables at s = lambda\n  # computes mle as test statistic at s = 0\n  mle <- foreach(i = 1:nRep, .combine = rbind, .packages = \"glmnet\") %dopar% {\n    fit <- glmnet(x, y[, i], family = family, intercept = FALSE)\n    coeff <- as.matrix(\n      coef(\n        fit, s = c(0, lambda), exact = TRUE,\n        x = x, y = y[, i], family = family,\n        intercept = FALSE\n      )\n    )\n    selection <- SignSummary(coeff[-1, 2])\n    data.frame(\n      theta1 = coeff[2, 1], theta2 = coeff[3, 1],\n      selection = selection\n    )\n  }\n  mle$selection <- as.factor(mle$selection)\n  \n  # computes the selection event based on the MLE at the first row\n  if (family == 'binomial') {\n    hessian <- HessianLogistic(x, as.numeric(mle[1, 1:2]))\n  } else if (family == 'gaussian') {\n    hessian <- HessianGaussian(x)\n  }\n  \n  # computes the segments of the polyhedron for event '00'\n  zero.seg <- -t(solve(hessian, rbind(c(1, -1, -1, 1), c(1, 1, -1, -1)))) *\n    lambda\n  zero.seg <- cbind(zero.seg, zero.seg[c(2, 3, 4, 1), ])\n  zero.seg <- data.frame(zero.seg)\n  # adds segment representing the other events\n  zero.seg <- rbind(\n    zero.seg,\n    zero.seg %>% dplyr::mutate(X3 = X1, X4 = sign(X2) * 10),\n    zero.seg %>% dplyr::mutate(X4 = X2, X3 = sign(X1) * 10)\n  )\n  \n  # computes the selection event based on true theta\n  if (family == 'binomial') {\n    true.hessian <- HessianLogistic(x, theta)\n  } else if (family == 'gaussian') {\n    true.hessian <- hessian\n  }\n  \n  # computes the segments of the polyhedron for event '00'\n  true.zero.seg <- -t(solve(true.hessian, rbind(c(1, -1, -1, 1), c(1, 1, -1, -1)))) *\n    lambda\n  true.zero.seg <- cbind(true.zero.seg, true.zero.seg[c(2, 3, 4, 1), ])\n  true.zero.seg <- data.frame(true.zero.seg)\n  # adds segment representing the other events\n  true.zero.seg <- rbind(\n    true.zero.seg,\n    true.zero.seg %>%\n      dplyr::mutate(X3 = X1, X4 = sign(X2) * 10),\n    true.zero.seg %>%\n      dplyr::mutate(X4 = X2, X3 = sign(X1) * 10)\n  )\n  \n  ggplot(mle) +\n    geom_point(aes(x = theta1, y = theta2, color = selection)) +\n    geom_point(x = mle[1, 1], y = mle[1, 2], shape = 4) +\n    geom_segment(\n      aes(x = X1, y = X2, xend = X3, yend = X4),\n      data = zero.seg, color = 'black'\n    ) +\n    geom_segment(\n      aes(x = X1, y = X2, xend = X3, yend = X4),\n      data = true.zero.seg, color = 'black',\n      linetype = 'dashed'\n    ) +\n    coord_cartesian(\n      xlim = c(min(mle$theta1), max(mle$theta1)),\n      ylim = c(min(mle$theta2), max(mle$theta2))\n    )\n}\n\nGLMSim(\n  theta = c(0.05, 0), lambda = 0.04, n = 100, nRep = 1000,\n  family = 'binomial'\n)\n\n\n\n\n\n\n\n\n\nWe then look specifically at the sample marked with x. The approximating the selection event based on the score at zero will approximate the event much better around the origin, but we also care much less about this scenerio. The ‘high stake’ scanerio is when the statistic is close the the boundaries — and in these cases we would want the selection event to be approximated better for that section of the boundary. The MLE thus appeals to this.\nThe Hessian of the log-likelihood has to be approximated as well. The selection event given by the true Hessian is given as dashed lines above, while the estimated Hessian is given as solid lines. Notice that while the approximation on the left edge of the red region is not done well, the approximation is done well in the bottom edge, which is more important to us. Also notice that the estimated Hessian performs fairly well.\nFinally, how is these approximations linked to that of Taylor and Tibshirani (2017)? They used the lasso estimate with one extra Newton step as their test statistic. Assuming the log-likelihood behaves sufficiently quadratic, this is the same as using the MLE. We admit that their approach probably has a slight edge, an MLE would require solving a whole new approximation problem, while a one-extra-Newton-step lasso estimate is extremely easy to compute. In application, we believe these two methods should perform similarly."
  },
  {
    "objectID": "posts/cal-fire.html",
    "href": "posts/cal-fire.html",
    "title": "Playing with Bayes and RStan",
    "section": "",
    "text": "It is hard to learn something new without any examples, and I happen to stumble upon this tweet:\n\n\nTrend line can be misleading, and there are good years too. But certainly it looks like bad years had become worse. Just curious, where can I find the data itself for educational purposes?\n\n— Kenneth Hung (@kenhungkk) September 7, 2020\n\n\nThe data itself is here and I got to learn how to handle PDFs with pdftools as well.\n\nlibrary(pdftools)\n\nUsing poppler version 22.06.0\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\nrstan (Version 2.21.5, GitRev: 2e1f913d3ca3)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\n\n\n\nAttaching package: 'rstan'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\ntheme_set(theme_minimal())\noptions(mc.cores = parallel::detectCores())\nregisterDoParallel(cores = parallel::detectCores())\n\nraw.text <- pdf_text(\n  'https://www.fire.ca.gov/media/11397/fires-acres-all-agencies-thru-2018.pdf'\n) %>%\n  str_split(\"\\n\") %>%\n  unlist()\n\ndata <- raw.text[4:35] %>%\n  as_tibble(.name_repair = \"unique\") %>%\n  mutate(\n    full.year = str_sub(value, end = 6), acres = str_sub(value, start = 132)\n  ) %>%\n  select(-value) %>%\n  mutate_all(str_trim) %>%\n  mutate_all(str_replace_all, \",\", \"\") %>%\n  mutate_all(as.numeric) %>%\n  mutate(year = full.year - min(full.year))\nN <- nrow(data)\n\nggplot(data) + geom_line(aes(x = full.year, y = acres))\n\n\n\n\n\n\n\n\nLike I said in the tweet, a linear fit seems off.\nUsing the default priors, I ran a Bayesian linear regression. I have to say, seeing the chains running and utilizing my new laptop’s computation power was very exciting.\n\nmodel.stan <- \"\ndata {\n  int<lower=0> N;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(alpha + beta * x, sigma);\n}\n\"\n\nfit <- stan(\n  model_code = model.stan,\n  data = list(N = N, x = data$year, y = data$acres),\n  iter = 5000\n)\n\nsummary(fit)$summary\n\n             mean      se_mean           sd        2.5%       25%         50%\nalpha 233861.6416 2.418253e+03 1.505192e+05 -56282.8026 133276.65 234057.1686\nbeta   25712.3024 1.386032e+02 8.440824e+03   8960.8308  20128.12  25641.7003\nsigma 446620.0347 8.779346e+02 6.010647e+04 347284.0777 403933.76 440538.9490\nlp__    -418.4848 2.105554e-02 1.253319e+00   -421.6139   -419.06   -418.1599\n              75%       97.5%    n_eff     Rhat\nalpha 333661.8810 529685.6716 3874.187 1.000570\nbeta   31297.4117  42305.1190 3708.713 1.000377\nsigma 481793.5035 581284.2707 4687.251 1.000244\nlp__    -417.5718   -417.0565 3543.156 1.000716\n\n\nSince all Bayesians do is do posterior draws, I don’t find it hard to understand the result. But what matters the most to me is that the data is fit well. As BDA would suggest, I should do a posterior predictive check, specifically something that would demonstrate my suspicion that linear model isn’t a good fit. By looking at the time series, I would guess that there are fewer positive residuals than negative ones. So I used the proportion of positive residuals after OLS as the test statistic. Embarrassingly it took me a long time to realize how to do a posterior predictive check for regression, the most basic example in Chapter 14 surprisingly did not emphasize this part.\n\npost <- extract(fit)\npost.pred.stat <- foreach(\n  alpha = post$alpha,\n  beta = post$beta,\n  sigma = post$sigma,\n  .combine = \"c\"\n) %dopar% {\n  y.rep <- alpha + beta * data$year + sigma * rnorm(N)\n  residual.rep <- residuals(lm(y.rep ~ data$year))\n  mean(residual.rep > 0)\n}\n\nresidual <- residuals(lm(acres ~ year, data = data))\nobs.stat <- mean(residual > 0)\n\nggplot() +\n  geom_histogram(\n    aes(x = post.pred.stat), alpha = 0.5, binwidth = 1 / nrow(data)\n  ) +\n  geom_vline(xintercept = obs.stat)\n\n\n\n\n\n\n\n\nIt looks like we have way too few positive residuals and I should probably use a log-linear model.\n\nmodel.stan <- \"\ndata {\n  int<lower=0> N;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(alpha + beta * x, sigma);\n}\n\"\n\nfit <- stan(\n  model_code = model.stan,\n  data = list(N = N, x = data$year, y = log(data$acres)),\n  iter = 5000\n)\n\nsummary(fit)$summary\n\n             mean      se_mean         sd         2.5%         25%         50%\nalpha 12.41468460 0.0045773388 0.27303403  11.88047622 12.23000357 12.41538394\nbeta   0.04188381 0.0002499936 0.01501446   0.01270324  0.03181958  0.04189893\nsigma  0.76954144 0.0014852081 0.10485453   0.59685152  0.69554526  0.75856354\nlp__  -7.15915278 0.0222561406 1.30575067 -10.58316010 -7.72086198 -6.82126935\n              75%       97.5%    n_eff     Rhat\nalpha 12.59478205 12.95319729 3558.012 1.001029\nbeta   0.05189791  0.07130844 3607.127 1.001190\nsigma  0.83173639  1.00139601 4984.250 1.000607\nlp__  -6.21397133 -5.68846914 3442.079 1.000834\n\n\nAnd we perform the same check to see if the residuals are symmetric.\n\npost <- extract(fit)\npost.pred.stat <- foreach(\n  alpha = post$alpha,\n  beta = post$beta,\n  sigma = post$sigma,\n  .combine = \"c\"\n) %dopar% {\n  y.rep <- alpha + beta * data$year + sigma * rnorm(N)\n  residual.rep <- residuals(lm(y.rep ~ data$year))\n  mean(residual.rep > 0)\n}\n\nresidual <- residuals(lm(log(acres) ~ year, data = data))\nobs.stat <- mean(residual > 0)\n\nggplot() +\n  geom_histogram(\n    aes(x = post.pred.stat), alpha = 0.5, binwidth = 1 / nrow(data)\n  ) +\n  geom_vline(xintercept = obs.stat)\n\n\n\n\n\n\n\n\nMuch better! Of course the model is not going to be correct, but we just need to keep checking for statistics that we care about. One idea I had is the number of times a new record is set. From the time series, it is 5 — we will count the very first year, not that it really matters. I thought this may be revealing if there is a lot of autocorrelation in the time series — for example, the more acres are burnt the previous year, the less there is to burn the year after.\n\npost <- extract(fit)\npost.pred.stat <- foreach(\n  alpha = post$alpha,\n  beta = post$beta,\n  sigma = post$sigma,\n  .combine = \"c\"\n) %dopar% {\n  y.rep <- alpha + beta * data$year + sigma * rnorm(N)\n  sum(y.rep == cummax(y.rep))\n}\n\nobs.stat <- sum(data$acres == cummax(data$acres))\n\nggplot() +\n  geom_histogram(\n    aes(x = post.pred.stat), alpha = 0.5, binwidth = 1\n  ) +\n  geom_vline(xintercept = obs.stat)\n\n\n\n\n\n\n\n\nNot too bad! For both model it looks like the slope is positive. There are many other data that would have been relevant to this analysis, such as the rainfall the year before and other climate data. There are also more sophisticated things such as Bayesian ARIMA that I could do (but I don’t know how), but hey, there are only 32 points in this dataset."
  },
  {
    "objectID": "posts/binomial-ranking.html",
    "href": "posts/binomial-ranking.html",
    "title": "Binomial ranking with SARS data",
    "section": "",
    "text": "With the coronavirus spreading to many countries, Rebecca asked me a curious question: how does the US perform during SARS compared to other regions in terms of survival rate? While we can compute the survival rate of all infected regions and rank them accordingly, we are ignoring the sampling variability. For example, South Africa that has one case but also one death, does not necessarily perform worse than Indonesia where there were two cases but both patients survived."
  },
  {
    "objectID": "posts/binomial-ranking.html#setup",
    "href": "posts/binomial-ranking.html#setup",
    "title": "Binomial ranking with SARS data",
    "section": "Setup",
    "text": "Setup\nOf course there are many other factors, but we can consider an idealized model where the number of patients, \\(n_i\\) in each country is predetermined, but the number of deaths, \\(X_i\\) is random and comes from a binomial draw:\n\\[X_i \\sim \\text{Binomial}(n_i, p_i),\\]\nwhere \\(p_i\\) represent the chance of a patient dying in region \\(i\\). We would then want to provide simultaneous confidence intervals for the rank of region \\(i\\), \\(r_i\\), as defined in Al Mohamad, Goeman, van Zwet (2018):\n\\[r_i = 1 + \\#\\{j \\ne i: p_j < p_i\\}\\]\nThere is an obvious Bayesian way to achieve this. By setting up a reasonable prior, we can perform posterior draws of \\(p_i\\) and search for confidence intervals of ranks that covers \\((1 - \\alpha)\\) of the posterior draws. Naturally this can be extended to an empirical Bayes way as well, as suggested in one of the comments in this Cross Validated thread. We want to focus on strict frequentist methods here."
  },
  {
    "objectID": "posts/binomial-ranking.html#data",
    "href": "posts/binomial-ranking.html#data",
    "title": "Binomial ranking with SARS data",
    "section": "Data",
    "text": "Data\nI have never done data scraping, so I am glad that this led me to learn rvest. We read in the table from the SARS page on Wikipedia. It looks like this:\n\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(ggplot2)\n  library(kableExtra)\n  library(rvest)\n  library(tidyr)\n})\n\ndata <- \"https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome\" %>%\n  read_html %>%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div/table[2]') %>%\n  html_table(fill = TRUE)\ndata <- data[[1]] %>%\n  setNames(c('region', 'cases', 'deaths', 'fatality', 'X1')) %>%\n  select(region, cases, deaths) %>%\n  filter(!grepl('total', tolower(region)), !grepl('\\\\^', region)) %>%\n  mutate(\n    region = trimws(gsub('\\\\[[[:print:]]\\\\]', '', region)),\n    cases = as.numeric(gsub(',', '', cases)),\n    deaths = as.numeric(gsub(',', '', deaths)),\n    fatality = deaths / cases\n  )\ndata %>% head() %>% kbl(format = \"markdown\")\n\n\n\n\nregion\ncases\ndeaths\nfatality\n\n\n\n\nChina\n5327\n349\n0.0655153\n\n\nHong Kong\n1755\n299\n0.1703704\n\n\nTaiwan\n346\n81\n0.2341040\n\n\nCanada\n251\n43\n0.1713147\n\n\nSingapore\n238\n33\n0.1386555\n\n\nVietnam\n63\n5\n0.0793651"
  },
  {
    "objectID": "posts/binomial-ranking.html#method-1-simultaneous-confidence-intervals",
    "href": "posts/binomial-ranking.html#method-1-simultaneous-confidence-intervals",
    "title": "Binomial ranking with SARS data",
    "section": "Method 1: Simultaneous confidence intervals",
    "text": "Method 1: Simultaneous confidence intervals\nOne way is to construct simultaneous confidence intervals for each of the region, and “project” to figure out the ranks. We implement those here:\n\n\nShow the code\numpu.expfam.test <- function(x, prob, u = runif(1)) {\n  x <- x - min(which(prob != 0)) + 1\n  prob <- prob[min(which(prob != 0)):max(which(prob != 0))]\n  n <- length(prob)\n  if (x > n | x <= 0) {\n    return(0)\n  }\n  \n  mean.x <- sum(prob * 1:n)\n  # observation is mean\n  if (abs(x - mean.x) < .Machine$double.eps^0.5) {\n    return(1 - u * prob[x])\n  }\n  # observation is on lower tail\n  if (x < mean.x) {\n    x <- n + 1 - x\n    mean.x <- n + 1 - mean.x\n    prob <- rev(prob)\n  }\n  \n  # dot product with this vector gives the covariance with x\n  cov.vec <- prob * (1:n - mean.x)\n  \n  prob.hi <- sum(prob[-(1:x)]) + prob[x] * u\n  cov.tail <- sum(cov.vec[-(1:x)]) + cov.vec[x] * u\n  cov.cumsum <- cumsum(cov.vec) + cov.tail\n  lo <- min(which(cov.cumsum < 0))\n  prob.lo <- sum(prob[1:lo]) - cov.cumsum[lo] / cov.vec[lo] * prob[lo]\n  \n  prob.lo + prob.hi\n}\n\numpu.binom.test <- function(x, n, p, u = runif(1)) {\n  umpu.expfam.test(x + 1, dbinom(0:n, n, p), u)\n}\n\numau.binom.ci <- function(x, n, alpha, u = runif(1)) {\n  f <- function(p) {\n    umpu.binom.test(x, n, p, u) - alpha\n  }\n  tol <- .Machine$double.eps^0.5\n  if (x == 0) {\n    ci.lo <- 0\n  } else {\n    ci.lo <- uniroot(f, c(0, x / n), tol = tol)$root\n  }\n  if (x == n) {\n    ci.hi <- 1\n  } else {\n    ci.hi <- uniroot(f, c(x / n, 1), tol = tol)$root\n  }\n  c(ci.lo, ci.hi)\n}\n\n# delta is the difference in log-odds\numpu.binom.contrast.test <- function(x1, n1, x2, n2, delta = 0, u = runif(1)) {\n  log.prob <- dbinom(0:(x1 + x2), n1, 0.5, log = TRUE) +\n    dbinom((x1 + x2):0, n2, 0.5, log = TRUE) +\n    0:(x1 + x2) * delta\n  log.prob <- log.prob - max(log.prob)\n  prob <- exp(log.prob)\n  prob <- prob / sum(prob)\n  umpu.expfam.test(x1 + 1, prob, u)\n}\n\n\nWe construct UMAU simultaneous confidence intervals:\n\nset.seed(20200215)\n\ndata.ci <- mapply(\n  umau.binom.ci,\n  data$deaths,\n  data$cases,\n  MoreArgs = list(alpha = 0.05 / nrow(data))\n) %>%\n  t %>%\n  data.frame %>%\n  setNames(c('ci.lo', 'ci.hi'))\ndata <- data %>% cbind(data.ci)\n\nggplot(\n  data,\n  aes(x = factor(region, region), y = fatality, ymin = ci.lo, ymax = ci.hi)\n) +\n  geom_point() +\n  geom_errorbar() +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = \"Region\", y = \"Fatality\")\n\n\n\n\n\n\n\n\nThese UMAU intervals are random by nature. Furthermore, the simultaneous coverage is achieved by Bonferroni correction here. A more fine-tuned analysis can be obtain by strategically distribute the type I error over the 29 regions, especially when some of these intervals are likely uninformative.\nWe can now compute the ranks for all parameters falling into the simultaneous confidence region\n\ndata %>%\n  mutate(\n    rank.lo = 1 + rowSums(outer(ci.lo, ci.hi, FUN = '-') >= 0),\n    rank.hi = rowSums(outer(ci.hi, ci.lo, FUN = '-') >= 0)\n  ) %>%\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\ncases\ndeaths\nfatality\nci.lo\nci.hi\nrank.lo\nrank.hi\n\n\n\n\nChina\n5327\n349\n0.0655153\n0.0553309\n0.0767850\n1\n27\n\n\nHong Kong\n1755\n299\n0.1703704\n0.1431314\n0.2001229\n2\n31\n\n\nTaiwan\n346\n81\n0.2341040\n0.1675014\n0.3108369\n2\n31\n\n\nCanada\n251\n43\n0.1713147\n0.1038446\n0.2564828\n2\n31\n\n\nSingapore\n238\n33\n0.1386555\n0.0791824\n0.2173352\n2\n31\n\n\nVietnam\n63\n5\n0.0793651\n0.0097358\n0.2311465\n1\n31\n\n\nUnited States\n27\n0\n0.0000000\n0.0000000\n0.1884739\n1\n31\n\n\nPhilippines\n14\n2\n0.1428571\n0.0001477\n0.5865567\n1\n31\n\n\nThailand\n9\n2\n0.2222222\n0.0012364\n0.7285998\n1\n31\n\n\nGermany\n9\n0\n0.0000000\n0.0000000\n0.5160983\n1\n31\n\n\nMongolia\n9\n0\n0.0000000\n0.0000000\n0.4660410\n1\n31\n\n\nFrance\n7\n1\n0.1428571\n0.0000000\n0.7410531\n1\n31\n\n\nAustralia\n6\n0\n0.0000000\n0.0000000\n0.5821487\n1\n31\n\n\nMalaysia\n5\n2\n0.4000000\n0.0022369\n0.9623212\n1\n31\n\n\nSweden\n5\n0\n0.0000000\n0.0000000\n0.7923318\n1\n31\n\n\nUnited Kingdom\n4\n0\n0.0000000\n0.0000000\n0.7418809\n1\n31\n\n\nItaly\n4\n0\n0.0000000\n0.0000000\n0.8810932\n1\n31\n\n\nBrazil\n3\n0\n0.0000000\n0.0000000\n0.8873430\n1\n31\n\n\nIndia\n3\n0\n0.0000000\n0.0000000\n0.9577680\n1\n31\n\n\nSouth Korea\n3\n0\n0.0000000\n0.0000000\n0.9277255\n1\n31\n\n\nIndonesia\n2\n0\n0.0000000\n0.0000000\n0.9982266\n1\n31\n\n\nSouth Africa\n1\n1\n1.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nColombia\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nKuwait\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nIreland\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nMacao\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nNew Zealand\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nRomania\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nRussia\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nSpain\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nSwitzerland\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31"
  },
  {
    "objectID": "posts/binomial-ranking.html#method-2-simultaneous-pairwise-tests",
    "href": "posts/binomial-ranking.html#method-2-simultaneous-pairwise-tests",
    "title": "Binomial ranking with SARS data",
    "section": "Method 2: Simultaneous pairwise tests",
    "text": "Method 2: Simultaneous pairwise tests\nBinomial distributions is an exponential family, so contrasts like \\(p_i - p_j\\) are amenable to UMPU tests. We can perform \\(\\binom{29}{2}\\) pairwise tests, with Bonferroni correction, and draw conclusions about the ranks.\n\nset.seed(113013)\n\npairwise.test <- by(\n  expand.grid(region1 = 1:nrow(data), region2 = 1:nrow(data)),\n  1:(nrow(data)^2),\n  function(pair) {\n    if (pair$region1 == pair$region2) {\n      return(1)\n    }\n    umpu.binom.contrast.test(\n      data$deaths[pair$region1],\n      data$cases[pair$region1],\n      data$deaths[pair$region2],\n      data$cases[pair$region2],\n      delta = 0\n    )\n  }\n) %>%\n  matrix(nrow(data), nrow(data))\npairwise.test[upper.tri(pairwise.test)] <- t(\n  pairwise.test\n)[upper.tri(pairwise.test)]\n\ndata %>%\n  select(-ci.lo, -ci.hi) %>%\n  mutate(\n    rank.lo = 1 +\n      rowSums(\n        (pairwise.test * choose(nrow(data), 2) < 0.05) &\n          (outer(data$fatality, data$fatality, FUN = \"-\") > 0)\n      ),\n    rank.hi = nrow(data) -\n      rowSums(\n        (pairwise.test * choose(nrow(data), 2) < 0.05) &\n          (outer(fatality, fatality, FUN = \"-\") < 0)\n      )\n  ) %>%\n  kbl(format = \"markdown\")\n\n\n\n\nregion\ncases\ndeaths\nfatality\nrank.lo\nrank.hi\n\n\n\n\nChina\n5327\n349\n0.0655153\n1\n28\n\n\nHong Kong\n1755\n299\n0.1703704\n2\n31\n\n\nTaiwan\n346\n81\n0.2341040\n2\n31\n\n\nCanada\n251\n43\n0.1713147\n2\n31\n\n\nSingapore\n238\n33\n0.1386555\n1\n31\n\n\nVietnam\n63\n5\n0.0793651\n1\n31\n\n\nUnited States\n27\n0\n0.0000000\n1\n31\n\n\nPhilippines\n14\n2\n0.1428571\n1\n31\n\n\nThailand\n9\n2\n0.2222222\n1\n31\n\n\nGermany\n9\n0\n0.0000000\n1\n31\n\n\nMongolia\n9\n0\n0.0000000\n1\n31\n\n\nFrance\n7\n1\n0.1428571\n1\n31\n\n\nAustralia\n6\n0\n0.0000000\n1\n31\n\n\nMalaysia\n5\n2\n0.4000000\n1\n31\n\n\nSweden\n5\n0\n0.0000000\n1\n31\n\n\nUnited Kingdom\n4\n0\n0.0000000\n1\n31\n\n\nItaly\n4\n0\n0.0000000\n1\n31\n\n\nBrazil\n3\n0\n0.0000000\n1\n31\n\n\nIndia\n3\n0\n0.0000000\n1\n31\n\n\nSouth Korea\n3\n0\n0.0000000\n1\n31\n\n\nIndonesia\n2\n0\n0.0000000\n1\n31\n\n\nSouth Africa\n1\n1\n1.0000000\n1\n31\n\n\nColombia\n1\n0\n0.0000000\n1\n31\n\n\nKuwait\n1\n0\n0.0000000\n1\n31\n\n\nIreland\n1\n0\n0.0000000\n1\n31\n\n\nMacao\n1\n0\n0.0000000\n1\n31\n\n\nNew Zealand\n1\n0\n0.0000000\n1\n31\n\n\nRomania\n1\n0\n0.0000000\n1\n31\n\n\nRussia\n1\n0\n0.0000000\n1\n31\n\n\nSpain\n1\n0\n0.0000000\n1\n31\n\n\nSwitzerland\n1\n0\n0.0000000\n1\n31\n\n\n\n\n\nHere we are bounded to correct for all \\(\\binom{29}{2}\\) tests, instead of taking advantage of Tukey’s HSD, which incurs a much smaller penalty for multiple testing. Asymptotically we can always think of the likelihood as if it came from a Gaussian distribution and use Tukey’s HSD, but we most definitely are not in any reasonable asymptotic regime here."
  },
  {
    "objectID": "posts/binomial-ranking.html#thoughts",
    "href": "posts/binomial-ranking.html#thoughts",
    "title": "Binomial ranking with SARS data",
    "section": "Thoughts",
    "text": "Thoughts\nAre there better, strictly frequentist methods for computing these rank confidence intervals? This alone seems hard, but there is a natural, even harder generalization of this problem: suppose we have a joint distribution of exponential families where the base measure does not need to be the same\n\\[p_i(X_i; \\theta_i) = h(x_i) \\exp(\\theta_i x_i - A_i(\\theta_i)),\\]\nis there a powerful method for ranking \\(\\theta_i\\)?"
  },
  {
    "objectID": "posts/nudge-meta-analysis.html",
    "href": "posts/nudge-meta-analysis.html",
    "title": "Meta-analysis on nudging experiments",
    "section": "",
    "text": "Does any “nudging” experiment have real effect? How many of the experiments?\nWhat happens if we counter the file-drawer effect with a more stringent threshold?\n\nSo first of all, we are going to only look at p-values that are smaller than \\(0.05\\), and adjust them by division by \\(0.05\\), as in our aforementioned paper, or Zhao, Small and Su (2018).\n\n\nShow the code\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(tidyr)\n})\n\nnudge_rawdata <- read.csv(\"https://osf.io/ubt9a/download/\")\n\nnudge_data <- nudge_rawdata %>%\n  select(publication_id, study_id, es_id, cohens_d, variance_d) %>%\n  mutate(\n    est = cohens_d,\n    sd = sqrt(variance_d),\n    z = est / sd,\n    p = 2 * pnorm(-abs(z))\n  )\n\nalpha <- 0.05\nadj_p <- nudge_data$p[nudge_data$p < alpha] / alpha\n\nhist(\n  adj_p, main = \"Histogram of adjusted p-values\", xlab = \"adjusted p-values\"\n)\n\n\n\n\n\n\n\n\n\nThe first question can be answered by our method, and for convenience, we will just pick \\(\\lambda\\) to be \\(0.5\\). All adjusted p-values will fall in one of these categories:\n\n\n\n\nNull\nNon-null\nTotal\n\n\n\n\nSmall (\\(p \\le 0.5\\))\n*\n*\n*\n\n\nBig (\\(p > 0.5\\))\n\\(U\\)\n*\n\\(B\\)\n\n\nTotal\n\\(V\\)\n*\n\\(R\\)\n\n\n\nSpecifically, we want to make inference on the false discovery proportion (FDP), which is \\(V / R\\), since every experiment was deemed a discovery beofre. Equivalently, \\(1 - V/R\\) would indicate the proportion of experiments that has an actual effect. Intuitively, if \\(B\\) is not that large, \\(U\\) cannot be large and \\(V\\) cannot be so big either. Formally, we will be using the inequality (2) from the paper:\n\\[B \\ge U \\ge_{\\text{st}} \\ge \\text{Binomial}(V, 1 - \\lambda),\\]\nand rejecting unlikely large values of \\(V\\). As a side, we can also get a point estimate of the FDP.\n\ninternal.comparison <- function(pval, lambda) {\n  R <- length(pval)\n  B <- sum(pval > lambda)\n  V_est <- B / (1 - lambda)\n  V <- 0:R\n  V_ucb <- V[max(which(pbinom(B, size = V, prob = 1 - lambda) > 0.05))]\n  data.frame(\n    R = R,\n    V_est = V_est,\n    V_ucb = V_ucb,\n    FDP_est = V_est / R,\n    FDP_ucb = V_ucb / R\n  )\n}\n\ninternal.comparison(adj_p, 0.5)\n\n    R V_est V_ucb   FDP_est   FDP_ucb\n1 269    74    90 0.2750929 0.3345725\n\n\nIn other words, at 95% confidence, at most 90 of the 269 are noise, meaning 179 of them have actual nonzero effect — but of course whether the effect is big enough to be meaningful is not up to me.\nWould these numbers look better if we use a more strigent threshold? It just so happens that our paper also has a method for it!\nWe will not be using all p-values this time, but for the used ones, they will fall into one of these cells in this table:\n\n\n\n\nNull\nNon-null\nTotal\n\n\n\n\nSmall (\\(p \\le \\alpha / 0.05\\))\n\\(V_\\alpha\\)\n\\(T_\\alpha\\)\n\\(R_\\alpha\\)\n\n\nBig (\\(p > 0.5\\))\n\\(U\\)\n\\(W\\)\n\\(B\\)\n\n\nTotal\n\\(N_0\\)\n*\n\\(N\\)\n\n\n\nThis time the inequality is more involved, from Lemma 1 in the paper (Did we make a typo in there?):\n\\[B = U + W \\ge_{\\text{st}} \\text{Binomial}\\left(N_0, \\frac{\\alpha / 0.05}{\\alpha / 0.05 + 0.5}\\right) + W \\ge_{\\text{st}} \\text{Binomial}(N - T_\\alpha, \\beta).\\]\nBut to use it, it is the same: we reject large values of \\(V_\\alpha\\) on the basis of small values of \\(B\\), and get a point estimate for this on the side.\n\n\nShow the code\nexternal.comparison <- function(pval, lambda, alpha) {\n  alpha <- alpha / 0.05\n  stopifnot(lambda > alpha)\n  beta <- (1 - lambda) / (1 - lambda + alpha)\n  R_alpha <- sum(pval < alpha)\n  B <- sum(pval > lambda)\n  V_alpha_est <- (1 - beta) / beta * B\n  N <- R_alpha + B\n  Qs <- 0:N\n  Q <- Qs[\n    max(which(pbinom(B, size = Qs, prob = beta) > 0.05))\n  ]\n  V_alpha_ucb <- Q - B\n  data.frame(\n    alpha = alpha * 0.05,\n    R = R_alpha,\n    V_est = V_alpha_est,\n    V_ucb = V_alpha_ucb,\n    FDP_est = V_alpha_est / R_alpha,\n    FDP_ucb = V_alpha_ucb / R_alpha\n  )\n}\n\nrbind(\n  external.comparison(adj_p, 0.5, 0.01),\n  external.comparison(adj_p, 0.5, 0.005),\n  external.comparison(adj_p, 0.5, 0.001)\n)\n\n\n  alpha   R V_est V_ucb    FDP_est    FDP_ucb\n1 0.010 195 14.80    23 0.07589744 0.11794872\n2 0.005 174  7.40    13 0.04252874 0.07471264\n3 0.001 127  1.48     4 0.01165354 0.03149606"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kenneth Hung",
    "section": "",
    "text": "I received my Ph.D. in mathematics at UC Berkeley, under Will Fithian. I focused on conditional inference and post-selection inference, as well as scientific replicability and mathematical statistics. My qualifying exam was on August 11 2016. The syllabus can be found here (pdf, tex).\nPrior to the Ph.D., I completed a B.S. with honors in mathematics with a computer science minor at Caltech. I was born in Toronto, Ontario and spent my formative years in Hong Kong and California.\nMy résumé and my c.v. are available here and here."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "multiple testing\n\n\n\n\nA recent meta-analysis on choice architecture, or “nudging”, but people do not seem to agree on if “nudging” works.\n\n\n\n\n\n\nAug 27, 2022\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\napplied statistics\n\n\n\n\nMoment conditions are common in literature, but how do we assess if they reasonably apply?\n\n\n\n\n\n\nApr 11, 2022\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistical computing\n\n\n\n\nFirst time pretending to be a Bayesian, and trying RStan.\n\n\n\n\n\n\nSep 25, 2020\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstatistical computing\n\n\n\n\nGetting the hang of statistical computing in Python\n\n\n\n\n\n\nAug 4, 2020\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmultiple testing\n\n\n\n\nIf deaths in each country follows a binomial distribution, how do we rank them by the probability parameter?\n\n\n\n\n\n\nFeb 15, 2020\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmultiple testing\n\n\nselective inference\n\n\n\n\nBetter multiple testing by screening p-values first, to reduce the penalty in common multiple testing procedure\n\n\n\n\n\n\nSep 21, 2018\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nselective inference\n\n\n\n\nGeneralizing Lasso penalty to non-linear model, e.g. a binomial model.\n\n\n\n\n\n\nApr 10, 2017\n\n\nKenneth Hung\n\n\n\n\n\n\nNo matching items"
  }
]