[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "UC Berkeley\n\nSpring 2019: GSI for Math 110 Linear Algebra with Olga Holtz\nFall 2018: GSI for Math 55 Discrete Mathematics with Mark Haiman\nSpring 2018: GSI for Math 53 Multivariable Calculus with Zvezdelina Stankova\nFall 2017: GSI for Math 1a Calculus with Vivek Shende\nFall 2016: GSI for Stat 210a Theoretical Statistics with Will Fithian\nSpring 2016: GSI for Math 1b Calculus with Alexander Paulin\nFall 2015: GSI for Math 55 Discrete Mathematics with David Li-Brand\nSummer 2015: Instructor for Math 54 Linear Algebra and Differential Equations with David Li-Brand\nSpring 2015: GSI for Math 1b Calculus with Alexander Coward\nFall 2014: GSI for Math 1a Calculus with Xinyi Yuan\n\nCaltech\n\nSpring 2014: TA for CS 38 Introduction to Algorithms with Chris Umans\nWinter 2014: TA for CS 21 Decidability and Tractability with Chris Umans"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Publications and Preprints\n\nEmpirical Bayes selection for value maximization. Dominic Coey and Kenneth Hung, 2022, preprint. (arxiv)\nCritical groups of strongly regular graphs and their generalizations. Kenneth Hung and Chi Ho Yuen, Innovations in Incidence Geometry: Algebraic, Topological and Combinatorial, 2022. (arxiv, iig)\nStatistical methods for replicability assessment. Kenneth Hung and William Fithian, Annals of Applied Statistics, 2020. (pdf, aoas, git)\nRank verification for exponential families. Kenneth Hung and William Fithian, Annals of Statistics, 2019. (pdf, aos, git)\n\nPresentations\n\n\n\nContent moderation and experimentation at Meta (invited joint talk at Simons Institute Annual Industry Day)\n\n\n\n\nEmpirical Bayes selection for value maximization (talk at CODE 2022)\n\n\n\n\nLarge-scale metric defense (poster at CODE 2021)\n\n\n\n\nStatistical methods for replicability assessment (invited talk at JSM 2021, invited talk at ISSI)\n\n\n\n\nRank verification for exponential families (talk at MCP 2017, poster at WHOA-PSI 2017)"
  },
  {
    "objectID": "posts/philosophy.html",
    "href": "posts/philosophy.html",
    "title": "Philosophy of statistics, or data science",
    "section": "",
    "text": "I’m catching up on my reading list finally and read Stark (2022) this weekend. I don’t think I understood all the subtleties in the paper, but it nonetheless made me take a step back and think a bit about what we are doing when we are doing statistics (or data science).\nIn my opinion, there are four main things covered in Stark (2022):"
  },
  {
    "objectID": "posts/philosophy.html#being-quantitative",
    "href": "posts/philosophy.html#being-quantitative",
    "title": "Philosophy of statistics, or data science",
    "section": "Being quantitative",
    "text": "Being quantitative\nWhen faced with different choices, a reasonably quantitative approach is to compare the change in total utility. Even if a decision involve multiple metrics with different scales and units, or different importance, all tradeoffs will comes down to a utility function.1 These different metrics correspond to the attributes in the sandwich example. Stark (2022) makes a good point that this assumption is deceptively innocuous. However, I don’t find the failure of the cancellation axiom convincing argument that we cannot find a utility function. Perhaps we should not expect the utility to be additive, and therefore, certain combinations (peanut butter with jelly) is just a lot better than peanut butter or jelly alone. The latter argument, that human generally prefers receiving $1m for sure than to receive $20m at a 10% chance, also didn’t convince me against a utility function.21 It does not mean that we know this function, but it at least allows us to assume local linearity, and justify tradeoffs.2 See this tweet and the discussion therein. For the record, I think the correct answer is to sell this button, or pool enough money from a large group of individuals so that clicking the green button makes sense.\nHowever I think the point stands, based on an argument against a sensible utility function from Mark Tygert. Kloumann and Tygert (2020) gave a good summary of Allais’s paradox: Consider two different decisions:\n\nchoose between a guaransteed 1 billion dollars, or getting 1 billion with probability 89%, 2 billion with probability 10% and nothing with probability 1%;\nchoose between 1 billion dollars at a 11% chance, or getting 2 billion at 10%.\n\nIf your decision is to pick the first option for decision 1, but the second option for decision 2, you are “inconsistent” even if we allow for potentially non-linear utility: Using \\(U(x)\\) to denote the utility for getting \\(x\\) billion dollars, then picking the first option for decision 1 amounts to\n\\[\\begin{eqnarray}\nU(1) & > & 0.89 \\cdot U(1) + 0.1 \\cdot U(2) + 0.01 \\cdot U(0) \\\\\n0.11 \\cdot U(1) & > & 0.1 \\cdot U(2) + 0.01 \\cdot U(0),\n\\end{eqnarray}\\]\nbut picking the second option for decision 2 amounts to\n\\[\\begin{eqnarray}\n0.11 \\cdot U(1) + 0.89 \\cdot U(0) & < & 0.1 \\cdot U(2) + 0.9 \\cdot U(0) \\\\\n0.11 \\cdot U(1) & > & 0.1 \\cdot U(2) + 0.01 \\cdot U(0).\n\\end{eqnarray}\\]\nThis can point either to our inability to do good calculation on-the-fly, or to the impossibility of a reasonable utility function here. Notice that we are not even looking at multiple metrics here in that there is one metric (money) involved, and so Stark’s point definitely stands."
  },
  {
    "objectID": "posts/philosophy.html#answering-the-wrong-question",
    "href": "posts/philosophy.html#answering-the-wrong-question",
    "title": "Philosophy of statistics, or data science",
    "section": "Answering the wrong question",
    "text": "Answering the wrong question\nStark (2022) then moves on to a disconnection between the scientific null hypothesis and a statistical null hypothesis. An example is analyzing how many bird the installation of wind turbines killed. An analyst may set up a zero-inflated Poisson regression model, and estimate some of the coefficients, which became the quantity of interest. While the coefficient may have little to do with birds killed, I do not think this as necessarily a bad thing: Anyone approaching this data has to start somewhere, and if we know why the coefficient carries no practical meaning, then we should have baked in the missing complexity in the model.\nI also have doubts on another example given: a standard randomized controlled test. The scientific null hypothesis is that there is no average treatment effect, while the statistical null hypotesis is that the sample mean of the two groups are independent and distributed normally, so that the t-test applies. Stark pointed out in the paper too, that a permutation test converges to a t-test under mild conditions. I cannot say for the others but I certainly have the scientific null hypothesis in mind when I run a t-test. The translation from a scientific null hypothesis to a statistical null hypothesis is always going to require extra modelling assumptions, and I’d say the assumptions that goes into a t-test is pretty barebone already.33 Except for cursed heavy-tailed data.\nFinally the paper introduced a concept displacement. It reminds me of reificiation in psychiatry (see e.g. Hyman 2010). Take schizophrenia as an example, we have some rough sense of what it entails, but the diagnosis mostly done by ratings. The rating mostly matches our intuition of what the disorder is, but carries no practical meaning. But it became the quantity of interest: medical treatment research would be done attempting to lower this rating, a decrease in rating is taken as treating schizophrenia. Like I said earlier, these analyses have to start somewhere, but I often wonder if it’s possible to treat the metric but not the disorder, analogous to Goodhart’s law."
  },
  {
    "objectID": "posts/philosophy.html#what-is-probability",
    "href": "posts/philosophy.html#what-is-probability",
    "title": "Philosophy of statistics, or data science",
    "section": "What is probability?",
    "text": "What is probability?\nThis was definitely a topic covered in grad school when we philosophized about statistics! Specifically the terms aleatory and epistemic were definitely used in one of the introductory grad level statistics classes. Aleatory refers to randomness coming from a mechanism, whereas epistemic refers to subjective randomness stemming from our ignorance. The distinction between the two types of probabilities seem to be clear to most, but quite blurry to me. A typical instance of aleatory probability would be a die roll. But to me, the unpredictability of the outcome is still a limtation on our knowledge or computation ability. The weight distribution of the die, or the aerodynamics as it is cast, can all be modelled and thus predicted. Perhaps except for quantum mechanics4, we have to take a stand on what information to include in a model and what to discard as randomness or “noise”.4 where we believe there is never precise knowledge of quantities\nAnother interesting perspective is rates vs probability, “but the mere fact that something has a rate does not mean that it is the result of a random process”. Probability is definitely a more precise concept than rates: if an event occurs with 1% probability, we will observe roughly one occurance every 100 trials. And perhaps the common argument for equating the two is ignorance. I really liked the question quoted from LeCam, “what is the probability that the \\(10^{137} + 1\\) digit of \\(\\pi\\) is a 7?” We would probably guess 10% but that is a non-sensical answer. No probability is involved in defining \\(\\pi\\) and so the probability really only comes from our own ignorance and a firm believe in some form of ergodicity. Curiously, this ergodicity is what powers all modern applications of probability and statistics via random number generator (RNG). We only think an RNG is uniformly random because it is supposed to be ergodic, or in the case of an RNG based on a hash function, we:\n\nrefuse to compute the inverse function;\nfeign ignorance of the seed;\nrely on histograms (read: rates) of the output, or histograms of short sequences of output, to claim uniformity.\n\nBut there still is nothing random about these output, given the seed the output is fully determined, like the \\(10^{137} + 1\\) digit of \\(\\pi\\)."
  },
  {
    "objectID": "posts/philosophy.html#some-models-are-rarely-falsified",
    "href": "posts/philosophy.html#some-models-are-rarely-falsified",
    "title": "Philosophy of statistics, or data science",
    "section": "Some models are rarely falsified",
    "text": "Some models are rarely falsified\nI think the only way a model can be falsified is by comparing its predictions to real outcomes. For models that give a lot of predictions, e.g. hourly or daily weather forecast, or stock price models, this should be easy. For models whose predictions are not that important, falsification is not a priority for humanity. So the models that Stark (2022) really wants to focus on must be ones that give few but consequential predictions. I think an example similar to climate change models would be election forecast: certainly consequential, and happens really only once. Predictions that come with error bars help, but the outcome that every one cares about (the winner of the presidential election) is a binary variable,5 where falsification only happens if the model assigned very high probability to one candidate. So perhaps falsification is also tied to the outcome of interest. And if the outcome of interest is hard to falsify, why do we bother with predicting that?5 at least in a two-party system"
  },
  {
    "objectID": "posts/p-value-screening.html",
    "href": "posts/p-value-screening.html",
    "title": "p-value screening",
    "section": "",
    "text": "Will and I have been working on a problem for finding confidence lower bounds for maximum parameter. Here is a good toy model: the confidence lower bound for the maximum mean parameter for an i.i.d. Gaussian observation. Specifically, we are trying to find a confidence bound for \\(\\max_i \\mu_i\\) from the observations \\(X \\sim N(\\mu, I_n)\\) where \\(\\mu\\) is the mean vector from \\(\\mathbb{R}^n\\).\nNow a good lower confidence bound would one that is larger. We can consider the test dual to the confidence bound: a test with higher power generally should mean a better confidence bound. The dual test will test the null hypothesis\n\\[H_0^\\tau: \\max_i \\mu_i \\le \\tau,\\]\nwhich is really just the intersection of \\(H_{0, i}^\\tau: \\mu_i \\le \\tau\\). We are now in the realm of multiple testing!\nA basic idea to test \\(H_0^\\tau\\) is of course through Bonferroni correction. However, as we test \\(H_0^\\tau\\) for \\(\\tau\\) that is larger than many of the observations \\(X_i\\)’s, few of these observations will provide evidence against the null but they count towards the multiplicity nonetheless. This is where we can aim to do better.\nZhao et al. (2018) suggested to use a \\(\\lambda\\) to screen out the p-values under \\(\\lambda\\), divide the remaining p-values by \\(\\lambda\\). (This idea was also rediscovered by Ellis et al. (2020).) We have also rediscovered the same idea, and all three groups stated very similar conditions for the p-values, from “uniformly conservative” to “supreuniform”. This \\(\\lambda\\) can also be taken as a stopping time: \\(\\lambda\\) can go from 0 to 1, revealing any p-value greater than \\(\\lambda\\) and stopping based only on this information. It can be shown that this still controls the type I error rate, through a martingale argument not unlike the proof of Benjamini–Hochberg procedure.\nBut Zhao, Small and Su (2018) took an extra step in proposing a beautiful method for selecting this \\(\\lambda\\): Note that the “saving” ones gets from performing the p-value screening is\n\\[\\frac{F(\\lambda)}{\\lambda},\\]\nwhere \\(F(\\lambda)\\) is the averaged distribution of the p-values. The smaller the ratio above, the better the test will perform. Taking the derivatives give\n\\[\\frac{d}{\\lambda} \\frac{F(\\lambda)}{\\lambda} = \\frac{F'(\\lambda) \\lambda - F(\\lambda)}{\\lambda^2},\\]\nand a good choice of stopping is when there is no strong evidence that \\(F'(\\lambda) \\lambda - F(\\lambda) > 0\\). In general this should eventually happen, either when \\(\\lambda\\) gets uncomfortably close to 0, or when most of the true nulls have been removed.\nThis approach is great for testing, but comes with two caveats when it comes to finding lower confidence bounds: - We do not know if this test is monotone: it is possible that the test rejects at \\(\\tau\\) yet accepts at some \\(\\tau' < \\tau\\). This may happen if the multiplicity correction is substantially greater for testing \\(H_0^{\\tau'}\\) than at \\(H_0^{\\tau}\\). - If we are not using the Zhao, Small and Su (2018) method for picking \\(\\lambda\\) and leaving this to an analyst’s discretion, the analyst may not be principled enough. They may be using a \\(\\lambda\\), while wishing that they have used stopped earlier.\nThe later point raises a question, similar to that in Johari et al. (2019) or “spotting time” as suggested by Aaditya Ramdas: Is it possible to allow an analyst stop anytime they want? If so, allowing an analyst to go backwards should come with a price. How do we make an appropriate correction?\nChoosing a screening threshold at \\(\\lambda\\) is essentially the same as using\n\\[n p_{(1)} \\frac{F_n(\\lambda)}{\\lambda}\\]\nas the test statistic and reject for small values. Here \\(p_{(1)}\\) is the smallest p-value and \\(F_n\\) is the empirical CDF of the p-values. What we want to do is essentially using the test statistic\n\\[n p_{(1)} \\min_{\\lambda \\in \\Lambda_0} \\frac{F_n(\\lambda)}{\\lambda}\\]\nfor some subset \\(\\Lambda_0 \\subset (0, 1]\\), and rejecting for small values. This is equivalent to allow an analyst to search through \\(\\Lambda_0\\) to cherry pick the best looking \\(\\lambda\\). Some possible options of \\(\\Lambda_0\\) are \\([\\lambda_0, 1]\\) for some prespecified \\(\\lambda_0\\) or \\(\\Lambda_0 = [p_{(k)}, 1]\\) where \\(p_{(k)}\\) is the \\(k\\)-th smallest p-value.\nThe first one, while maybe more practical, is less interesting as it probably is not too different from using a fixed \\(\\lambda_0\\). We will turn to the second option here. We will analyse it by finding a “stochastic lower bound” for this statistic. We have\n\\[\\begin{eqnarray}\nn p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda)}{\\lambda} &=& n p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F(\\lambda)}{\\lambda} \\frac{F_n(\\lambda)}{F(\\lambda)} \\\\\n&\\stackrel{\\text{st}}{\\ge}& n p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F(\\lambda)}{\\lambda} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda)}{F(\\lambda)} \\\\\n&=& n p_{(1)} \\frac{F(p_{(1)})}{p_{(1)}} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda)}{F(\\lambda)} \\\\\n&=& n F(p_{(1)}) \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda)}{F(\\lambda)}.\n\\end{eqnarray}\\]\nOf course, \\(n F(p_{(1)})\\) is stochastically larger than \\(U[0, 1]\\). So it suffices to control \\(\\min_{\\lambda \\ge p_{(k)}} F_n(\\lambda) / F(\\lambda)\\), or equivalently, make sure \\(\\max_{\\lambda \\ge p_{(k)}} F(\\lambda) / F_n(\\lambda)\\) is not too big. We consider the least favorable distribution, where all p-values are uniformly distributed, i.e. \\(F(p) = p\\).\nThis quantity of interest may look like something from empirical process, but we can focus on its value at one of the p-values. So it is good enough to look at\n\\[F(p_{(t)}) / F_n(p_{(t)}) = n p_{(t)} / t,\\]\nwhich happens to be a martingale under the filtration \\(\\mathcal{F}_t = \\sigma(\\{p_{(t)}, \\ldots, p_{(n)}\\})\\) as \\(t\\) decreases from \\(n\\) to \\(k\\). The expectation of each term in \\(n / (n+1)\\), so we can use Doob’s martingale inequality on the submartingale\n\\[\\left(\\frac{p_{(t)}}{t} - \\frac{n}{n+1}\\right)^2\\]\nto get some concentration, giving\n\\[\\begin{eqnarray}\n\\mathbb{P}\\left[\\max_{k \\le t \\le n} \\left(\\frac{n p_{(t)}}{t} - \\frac{n}{n+1}\\right)^2 > C\\right] &\\le& \\frac{\\mathrm{var}[n p_{(k)} / k]}{C} \\\\\n&=& \\frac{1}{C} \\cdot \\frac{n^2 (n+1-k)}{k (n+1)^2 (n+2)}.\n\\end{eqnarray}\\]\nThere probably is a better bound, but for now let’s stick with this. Under the null we have\n\\[\\begin{eqnarray}\n\\mathbb{P}\\left[\\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda) + \\frac{1}{n}}{\\lambda} < \\frac{1}{C}\\right] &\\le& \\mathbb{P}\\left[\\max_{k \\le t \\le n} \\frac{p_{(k)}}{k} < C\\right] \\\\\n&\\le& \\frac{1}{\\left(C - \\frac{n}{n+1}\\right)^2} \\cdot \\frac{n^2 (n+1-k)}{k (n+1)^2 (n+2)}\n\\end{eqnarray}\\]\nfor any \\(C > \\frac{n+1}{n}\\). With this bound, we have that\n\\[C n p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda) + \\frac{1}{n}}{\\lambda} + \\frac{1}{\\left(C - \\frac{n}{n+1}\\right)^2} \\cdot \\frac{n^2 (n+1-k)}{C k (n+1)^2 (n+2)}\\]\nis stochastically larger than \\(U[0, 1]\\) for any \\(C > \\frac{n}{n+1}\\). Optimizing over \\(C\\) gives that\n\\[\\frac{n^2}{n+1} p_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda) + \\frac{1}{n}}{\\lambda} + \\frac{3}{2^{2/3}} \\left(np_{(1)} \\min_{\\lambda \\ge p_{(k)}} \\frac{F_n(\\lambda) + \\frac{1}{n}}{\\lambda}\\right)^{2/3} \\left(\\frac{n^2 (n+1-k)}{k (n+1)^2 (n+2)}\\right)^{1/3}\\]\nis also stochastically larger than \\(U[0, 1]\\), so we can use this as our p-value.\nIn a pessimistic case where the analyst shows no restraint, there is nevertheless no reason to choose \\(\\lambda \\le p_{(2)}\\), so the smallest \\(k\\) is 2. Now the question comes: how does this test fare compared to Zhao, Small, Su (2018)?\nWe wrote a short piece of code to test this:\n\nsuppressPackageStartupMessages(library(foreach))\n\nspotting.test <- function(x, k = 2) {\n  n <- length(x)\n  p <- sort(pnorm(x, lower.tail = FALSE))\n  multiplier <- min((k:n) / p[k:n] / n)\n  comb.p <-\n    n^2 / (n + 1) * p[1] * multiplier +\n    3 / 2^(2 / 3) * (n * p[1] * multiplier)^(2 / 3) *\n    (n^2 * (n + 1 - k) / k / (n + 1)^2 / (n + 2))^(1 / 3)\n  min(comb.p, 1)\n}\n\nspotting.power <- function(mu, k = 2) {\n  require(foreach)\n  rej <- foreach(i = 1:10000, .combine = \"c\") %do% {\n    x <- mu + rnorm(length(mu))\n    spotting.test(x, k) < 0.05\n  }\n  mean(rej)\n}\n\nc(\n  spotting.power(rep(0, 100)),\n  spotting.power(c(4, rep(0, 99))),\n  spotting.power(c(4, rep(-1, 99))),\n  spotting.power(c(4, rep(-4, 99))),\n  spotting.power(c(4, rep(-10, 99))),\n  spotting.power(c(rep(1, 20), rep(0, 80))),\n  spotting.power(c(rep(1, 20), rep(-1, 80))),\n  spotting.power(c(rep(1, 20), rep(-4, 80)))\n)\n\n[1] 0.0076 0.5725 0.7113 0.8851 0.8873 0.0440 0.0621 0.1157\n\n\nSince post condition we are really just using Bonferroni test, we will compare to those particular rows in Table 2 in Zhao, Small, Su (2018):\n\n\n\n\n\n\n\n\n\nSetting\n\\(\\tau\\) = 0.5\nAdaptive\nSpotting\n\n\n\n\n1. All null\n5.0\n5.0\n0.7\n\n\n2. 1 strong 99 null\n76.6\n76.7\n57.3\n\n\n3. 1 strong 99 conservative\n85.2\n84.0\n70.9\n\n\n4. 1 strong 99 very conservative\n98.0\n98.7\n88.3\n\n\n5. 1 strong 99 extremely conservative\n97.8\n98.9\n89.0\n\n\n6. 20 weak 80 null\n21.0\n22.5\n4.6\n\n\n7. 20 weak 80 conservative\n28.1\n26.3\n6.0\n\n\n8. 20 weak 80 very conservative\n38.1\n47.3\n11.6\n\n\n\nWelp. This does not work that well. One possible reason is that “Adaptive” does a good job capturing the best cutoff already, spotting needs to account for too much noise and pays an unnecessarily high price. In either case, it is not clear if the spotting test is monotone anyway.\n\n\n\n\nReferences\n\nEllis, J. L., Pecanka, J., and Goeman, J. J. (2020), “Gaining power in multiple testing of interval hypotheses via conditionalization,” Biostatistics, 21, e65–e79.\n\n\nJohari, R., Pekelis, L., and Walsh, D. J. (2019), “Always Valid Inference: Bringing Sequential Analysis to A/B Testing,” arXiv. https://doi.org/10.48550/arXiv.1512.04922.\n\n\nZhao, Q., Small, D. S., and Su, W. J. (2018), “Multiple testing when many p-values are uniformly conservative, with application to testing qualitative interaction in educational interventions,” Journal of the American Statistical Association."
  },
  {
    "objectID": "posts/parametric-mean.html",
    "href": "posts/parametric-mean.html",
    "title": "Parametric mean estimation",
    "section": "",
    "text": "A common task: Mean estimation\nSuppose you are given i.i.d. samples \\(X\\) drawn from some unknown distribution and we wish to estimate its mean, \\(\\mathbb{E}[X]\\). However being a good data scientist, you inspect the distribution \\(X\\) first, and saw the following histogram.\n\n\nShow the code\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\nset.seed(1)\nx <- exp(rnorm(1e4))\nggplot() + geom_histogram(aes(x = x))\n\n\n\n\n\nThat was not a great histogram that tells us little, since everything is just on the left. We decided to apply log transform and try again.\n\n\nShow the code\nggplot() + geom_histogram(aes(x = log(x)))\n\n\n\n\n\nNow things are looking normal. But wait, we are supposed to find \\(\\mathbb{E}[X]\\), not \\(\\mathbb{E}[\\log(X)]\\). Sample mean looks like it has a lot of variance. On the other hand, the log-transformed samples are looking really normally distributed. If we are willing to believe that \\(\\log(X) \\sim N(\\mu, \\sigma^2)\\), can we do better?\n\n\nSetup\nSuppose we want to model \\(X\\) as \\(X = h(Z)\\) for some \\(Z \\sim N(\\mu, \\sigma^2)\\). This is perhaps believable after staring at histograms for hours, or perhaps \\(X\\) themselves are suppose to be normally distributed by Central Limit Theorem, but it’s not quite there yet. In the example above, \\(h(Z) = \\exp(Z)\\).\nNow we have two estimators of \\(\\mathbb{E}[X]\\). One is sample mean (the non-parametric one), \\[\\frac{1}{n} \\sum_{i = 1}^n X_i,\\] and the other one is the parametric one \\[f(\\hat{\\mu}, \\hat{\\sigma}) \\coloneqq \\mathbb{E}_{Z \\sim N(\\hat{\\mu}, \\hat{\\sigma}^2)}[h(Z)],\\] where \\((\\hat{\\mu}, \\hat{\\sigma})\\) can be estimated via your favorite method which is probably maximum-likelhood in this case.\nUnless \\(h\\) is something weird, both are probably consistent. The parametric estimator may be biased, but that is a risk I am willing to take. How are their efficiencies?\nIntuitively, the parametric one should be more efficient. We are supplying extra structure to the problem (the parametrization) and giving up unbiasedness. We must be gaining something, right?\n\n\nEfficiency\nLet’s analyze the simpler one, the non-parametric estimator. The asymptotic variance, scaled appropriately by \\(n\\), is \\(\\text{var}(h(Z)) = \\mathbb{E}[h(Z)^2] - [\\mathbb{E} h(Z)]^2\\).\nFor the parametric one, the asymptotic variance of maximum likelihood estimator \\((\\hat{\\mu}, \\hat{\\sigma})\\) is the inverse of the Fisher information matrix \\[\\begin{pmatrix} \\sigma^2 & 0 \\\\ 0 & \\sigma^2 / 2\\end{pmatrix}.\\] The estimator \\((\\hat{\\mu}, \\hat{\\sigma})\\) is also asymtotically normal, so we can apply delta method to get the asymptotic variance of \\(f(\\hat{\\mu}, \\hat{\\sigma})\\). The remaining work is to find the gradient of \\(f\\).\nWe write the partial derivative out, swap it with the integral, then apply Stein’s lemma to get \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\mu} \\mathbb{E}[h(Z)] &= \\frac{\\partial}{\\partial \\mu} \\int h(z) \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(z - \\mu)^2}{2 \\sigma^2}\\right) \\,dz \\\\\n&= \\int h(z) \\frac{1}{\\sigma \\sqrt{2\\pi}} \\frac{z - \\mu}{\\sigma^2} \\exp(\\cdots) \\,dz \\\\\n&= \\frac{1}{\\sigma^2} \\mathbb{E}[h(Z)(Z - \\mu)] \\\\\n&= \\mathbb{E}[h'(Z)].\n\\end{align*}\n\\] In retrospect, I probably could have skipped the integral, since moving \\(\\mu\\) is the same as moving \\(X\\).\nLet’s try to be smarter with the other partial derivative. \\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\sigma} \\mathbb{E}[h(Z)] &= \\lim_{\\tau \\to \\sigma} \\frac{1}{\\tau - \\sigma} \\left[\\mathbb{E}_\\tau[h(Z)] - \\mathbb{E}_\\sigma[h(Z)]\\right] \\\\\n&= \\lim_{\\tau \\to \\sigma} \\frac{1}{\\tau - \\sigma} \\left[\\mathbb{E}_\\sigma\\left[h\\left(\\frac{\\tau}{\\sigma} Z + \\frac{\\sigma - \\tau}{\\sigma} \\mu\\right)\\right] - \\mathbb{E}_\\sigma [h(Z)]\\right] \\\\\n& = \\lim_{\\tau \\to \\sigma} \\frac{1}{\\tau - \\sigma} \\left[\\mathbb{E}\\left[h'(Z) \\frac{\\tau - \\sigma}{\\tau} (Z - \\mu)\\right]\\right] \\\\\n& = \\frac{1}{\\sigma} \\mathbb{E}[h'(Z) (Z - \\mu)] \\\\\n& = \\sigma \\mathbb{E}[h''(Z)].\n\\end{align*}\n\\]\nSo the asymptotic variance, scaled again by \\(n\\), is \\[\n\\sigma^2 [\\mathbb{E} h'(Z)]^2 + \\frac{\\sigma^4}{2} [\\mathbb{E} h''(Z)]^2.\n\\]\nHere are some simple cases.\n\n\n\n\n\n\n\n\n\\(h(z)\\)\nNon-parametric\nParametric\n\n\n\n\n\\(z\\)\n\\(\\sigma^2\\)\n\\(\\sigma^2\\)\n\n\n\\(z^2\\)\n\\(4 \\mu^2 \\sigma^2 + 2 \\sigma^4\\)\n\\(4 \\mu^2 \\sigma^2 + 2 \\sigma^4\\)\n\n\n\\(z^3\\)\n\\(9 \\mu^4 \\sigma^2 + 36 \\mu^2 \\sigma^4 + 15 \\sigma^6\\)\n\\(9 \\mu^4 \\sigma^2 + 36 \\mu^2 \\sigma^4 + 9 \\sigma^6\\)\n\n\n\\(z^4\\)\n\\(16 \\mu^6 \\sigma^2 + 168 \\mu^4 \\sigma^4 + 384 \\mu^2 \\sigma^6 + 96 \\sigma^8\\)\n\\(16 \\mu^4 \\sigma^2 + 132 \\mu^4 \\sigma^4 + 216 \\mu^2 \\sigma^4 + 36 \\sigma^8\\)\n\n\n\\(\\exp(z)\\)\n\\((e^{\\sigma^2} - 1)\\exp(2\\mu + \\sigma^2)\\)\n\\((\\sigma^2 + \\sigma^4 / 2) \\exp(2\\mu + \\sigma^2)\\)\n\n\n\nWhile the parametric variance is smaller, it is hard to say how much we gain from trading away the unbiasedness. If \\(\\sigma / \\mu \\ll 1\\), then these two are really the same. But in that case, delta method gives a good approximation and \\(X\\) would pretty much look normal.\n\n\nQuestions\n\nIs the non-parametric variance always at least that of the parametric one? It looks like that is the case above, but I do not know for sure.\nIt makes sense that the two variances are the same for \\(h(z) = z\\), but why is that the case for \\(h(z) = z^2\\) as well? Is there something special about taking squares?"
  },
  {
    "objectID": "posts/rnorm-in-python.html",
    "href": "posts/rnorm-in-python.html",
    "title": "Normal distribution in Python",
    "section": "",
    "text": "Working on theoretical statistics, all of my work in my PhD was done in R. But for production code in industry, for the sake of speed and easier maintenance, I have taken up to implement some of my ideas in Python even when the prototyping is done in R. I did not expect to be learning this much by just looking at a normal distribution."
  },
  {
    "objectID": "posts/rnorm-in-python.html#scipy.stats-vs-math",
    "href": "posts/rnorm-in-python.html#scipy.stats-vs-math",
    "title": "Normal distribution in Python",
    "section": "scipy.stats vs math",
    "text": "scipy.stats vs math\nSuppose we want to compute the CDF of a standard normal. An R user like me who is used to pnorm(z) would probably write this\nfrom scipy.stats import norm\n\nnorm.cdf(z)\nBut another way is to use the erf function, given by\n\\[\n\\text{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z e^{-t^2} \\,dt.\n\\]\nWe can then write\n\nfrom math import erf\n\ndef phi(z):\n  return (1.0 + erf(z / sqrt(2.0))) / 2.0\n\nIn fact this is the given example in the Python documentations for the math library.\nFor my purpose, this function needs to be run many times, so speed is definitely important here. We run it for 10000 times:\n\nfrom timeit import default_timer\nfrom scipy.stats import norm\nfrom math import erf\n\ndef phi(z):\n  return (1.0 + erf(z / 1.4142135623730951)) / 2.0\n\nstart = default_timer()\nscipy_output = [norm.cdf(1) for i in range(10000)]\nend = default_timer()\nprint(\"scipy took:\", str(end - start))\n\nscipy took: 0.9739241569999999\n\nstart = default_timer()\nmath_output = [phi(1) for i in range(10000)]\nend = default_timer()\nprint(\"math took:\", str(end - start))\n\nmath took: 0.011614696000000091\n\n\nComputing using erf was a lot faster than using scipy, but that should not come as a surprise. Even in R, a loop without any vectorization is bound to be slow. After all, the strength of scipy is that it works well with numpy arrays. So let’s vectorize it:\n\nimport numpy\n\nstart = default_timer()\nnorm.cdf(numpy.repeat(1, 10000))\n\narray([0.84134475, 0.84134475, 0.84134475, ..., 0.84134475, 0.84134475,\n       0.84134475])\n\nend = default_timer()\nprint(\"scipy took:\", str(end - start))\n\nscipy took: 0.013408950000000086\n\n\nToo bad that in my use case, the CDF of the normal distribution has to be computed in a loop, so I will go with the erf route…"
  },
  {
    "objectID": "posts/rnorm-in-python.html#erf-vs-erfc",
    "href": "posts/rnorm-in-python.html#erf-vs-erfc",
    "title": "Normal distribution in Python",
    "section": "erf vs erfc",
    "text": "erf vs erfc\n…which brings us to another question. How accurate is computing the CDF based on erf? The part where we add erf to 1.0 means that anything that is close to or smaller than the machine epsilon will get erased. What if we do care about the magnitude of the CDF far in the tail of the normal? With the time constraint, we cannot really call norm.logcdf here.\n\n[phi(-z) for z in range(5, 15)]\n\n[2.8665157186802404e-07, 9.865876449133282e-10, 1.2798095916366492e-12, 6.106226635438361e-16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n\n\nWe can instead use erfc, defined as\n\\[\n\\text{erfc}(z) = 1 - \\text{erf}(z).\n\\]\nNow we have\n\nfrom math import erfc\n\ndef phi(z):\n  return erfc(-z / 1.4142135623730951) / 2.0\n\n[phi(-z) for z in range(5, 15)]\n\n[2.866515718791945e-07, 9.865876450377014e-10, 1.2798125438858348e-12, 6.22096057427182e-16, 1.1285884059538425e-19, 7.619853024160593e-24, 1.9106595744986828e-28, 1.7764821120777016e-33, 6.117164399549921e-39, 7.793536819192798e-45]\n\n\nIt preserved many more digits. It is curious how the Python documentations chose to use this example for erf instead of erfc. To check if this is correct, I run in R:\n\npnorm(-(5:14))\n\n [1] 2.866516e-07 9.865876e-10 1.279813e-12 6.220961e-16 1.128588e-19\n [6] 7.619853e-24 1.910660e-28 1.776482e-33 6.117164e-39 7.793537e-45\n\n\nwhich gave the exactly same numbers. Did R perhaps implement pnorm using erfc as well?"
  },
  {
    "objectID": "posts/moment-conditions.html",
    "href": "posts/moment-conditions.html",
    "title": "Moment conditions heuristics",
    "section": "",
    "text": "A theorem in statistics is a theorem in mathematics, and of course the conclusion only really holds if the requirements hold. I can’t say much about Bayesian literaure, but the papers I have been reading recently all have moment conditions. A classic exmaple is of course the (Lindeberg-Lévy) Central Limit Theorem, where\n\\[\\text{var}(X_i) = \\sigma^2 < \\infty.\\]\nThis classic formulation of Central Limit Theorem is often used in estimating the population mean when we only have a small sample, where we get a point estimate and an estimated standard error, then apply the normal approximation. But how do we know if the theorem “applies”? I’ve usually gotten two answers, or a combination of both: - If you are so worried about the normal approximation, use bootstrap. - The theorem always apply, since you technically have a finite population, so you technically have a finite variance regardless. - (Occasionally) The theorem always apply, and how well the approximation is governed by Berry–Esseen theorem. (I guess this is somewhat helpful, but then I’m stuck figuring out what the third moment is.)\nFor the first answer, my intuition is that when normal approximation is poor, bootstrap (at least standard percentile bootstrap) will be poor too, since the proofs of bootstrap I am familiar with goes through normal distributions too (e.g. van der Vaart, Chapter 23.2.1).\nFor the second answer, I still want to know if the approximation is good. Philosophically, this is similar to doing high-dimensional linear regression — when we have \\(n = 1000\\) and \\(p = 100\\), should we be looking at results on \\(p = O(n)\\) or \\(p = O(1)\\)? The idea here is the same: when we look at our data, we can sometimes tell that the normal approximation is going to fail if the top 10% of your sample accounts adds up to 90% of the sum. Can we have some heuristic that these moment conditions are going to fail?\nMore generally, I want to be able to look “empirically check” for moment conditions that look like\n\\[\\frac{1}{n} \\sum_{i=1}^n |X_i|^\\beta = o(n^\\gamma).\\]\nIn most cases I’ve encountered, \\(\\beta\\) would be an integer, and I am including a \\(\\gamma\\) there as well to include cases like Guo and Basse (2020), Example 1:\n\\[\\frac{1}{n} \\sum_{i=1}^n X_i^4 = o(n).\\]\nBelow are back-of-the-envelope calculations that I’ve been using recently to reason around these conditions. We are going to start with the stable distribution, with tail index \\(\\alpha\\). Recall that if \\(X, Y \\sim \\text{Stable}(\\alpha)\\) independently, then \\(X + Y \\sim 2^{1/\\alpha} \\cdot \\text{Stable}(\\alpha)\\). Now in cases where the generalized Central Limit Theorem works, we will be able to get, with some scaling and some choice of \\(\\alpha\\),\n\\[\\frac{1}{n/2} \\sum_{i=1}^{n/2} |X_i|^\\beta, \\frac{1}{n/2} \\sum_{i=n/2 + 1}^{n} |X_i|^\\beta \\sim \\text{Stable}(\\alpha),\\]\nand combining them will get\n\\[\\frac{1}{n} \\sum_{i=1}^n |X_i|^\\beta \\sim 2^{1/\\alpha - 1} \\cdot \\text{Stable}(\\alpha).\\]\nRepeating this ad infinitum, we have\n\\[\\frac{1}{n} \\sum_{i=1}^n |X_i|^\\beta = O_p((2^{1/\\alpha - 1})^{\\log_2 n}) = O_p(n^{1/\\alpha - 1}).\\]\nSo for example, in the case of Guo and Basse (2021), Example 1, we will need \\(X_i^\\beta\\) to be no more heavy-tailed than \\(\\alpha = 1/2\\). But now, since \\(X \\sim \\text{Stable}(\\alpha)\\) is roughly the boundary where \\(\\mathbb{E}[X^\\alpha]\\) exists, Guo and Basse (2021) should be roughly the same as asking the tail index to be at least 2, pretty much the same as asking that Central Limit Theorem applies.\nI think the heuristics above can be made a bit more rigorous too, probably through \\(\\mathbb{P}[X > n^a] > n^{-b}\\) for some choice of \\(a\\) and \\(b\\), these choices of \\(a\\) and \\(b\\) can perhaps be compared to a generalized Pareto distribution.\nFinally to check this, without getting into the subtleties of Hill estimator or Clauset et al. (2009), we can look at some high quantiles and take the ratio. For example, a rough estimate of the tail index would be \\(1 / \\log_{10}(q_{0.999} / q_{0.99})\\).\n\n\n\n\nReferences\n\nClauset, A., Shalizi, C. R., and Newman, M. E. J. (2009), “Power-Law Distributions in Empirical Data,” SIAM Review, 51, 661–703. https://doi.org/10.1137/070710111.\n\n\nGuo, K., and Basse, G. (2020), “The Generalized Oaxaca-Blinder Estimator.” https://doi.org/10.1080/01621459.2021.1941053."
  },
  {
    "objectID": "posts/asymptotic-selective-inference.html",
    "href": "posts/asymptotic-selective-inference.html",
    "title": "L1-penalized likelihood asymptotics",
    "section": "",
    "text": "Following a paper by Lee et al. (2016) on the correcting for the selection bias after lasso-based selection, a natural progression is to consider general penalized likelihood selections. In GLM, we are at least provided with a sufficient statistics, but this would not be the case in a more general likelihood setting, rendering the description of the selection event a lot more blurry.\nMost specifically, the set up is as follows: we have a matrix \\(X\\) consisting of \\(n\\) row of covariates, \\(Y\\) a vector consisting of all the responses. We assume the model, given by the log-likelihood below,\n\\[\\sum_{i=1}^n \\ell(\\theta_n; Y_n, X_n).\\]\nHere we will not move into high-dimensional regime and thus assumes \\(\\theta_n\\) to have dimension \\[d\\]. Subsequently, we perform selection based on maximizing\n\\[\\sum_{i=1}^n \\ell(\\theta_n; Y_n, X_n) - \\lambda_n \\|\\theta_n\\|_1,\\]\nwith some tuning parameter \\(\\lambda_n\\).\nAround three weeks ago, Will and I came up with a way to tackle this problem — together with a non-exhaustive, non-optimized list of conditions needed. Unfortunately shortly after Will discovered a paper by Taylor and Tibshirani (2017) that arrived at an almost identical solution. While our result might no longer be groundbreaking, we hope that this post will provide a different perspective from Taylor and Tibshirani (2017), and assist anyone who happens to also be reading Taylor and Tibshirani (2017).\nThe problem has two main hurdles: - approximating the selection event in a reasonable yet theoretically valid manner; - choosing a test statistic with a nice asymptotic distribution.\nWe can make both decisions at once by considering the selection event. In GLM, with a sufficient statistic, the selection event will always be measurable with respect to this sufficient statistic. This measurability requirement results in fuzzy edges if we plot out the selection event based on a non-sufficient statistic.\nWe don’t have this sufficient statistic anymore in a general likelihood setting. Conventionally, both the score at a fixed parameter and the MLE are thought of as ‘asymptotically sufficient’ without a proper definition. Since we are looking into asymptotics anyways, these two statistic seems perfect for our use. A bonus is that their asymptotic distributions are well known.\nFollowing classical asymptotic analysis as explained in van der Vaart (1998), we will assume that \\(\\theta_n = \\theta_0 + \\beta / \\sqrt{n}\\) and thus converges to a \\(\\theta_0\\) that lies in the null hypothesis \\(\\Theta_0\\). Other possible asymptotic regimes includes modifying the lasso minimization problem into a ‘non-centered’ lasso problem\n\\[\\sum_{i=1}^n \\ell(\\theta_n; Y_n, X_n) - \\lambda_n \\|\\theta_n - c_n\\|_1,\\]\nbut as it turns out the asymptotics will work out to the same solution anyways. For the lasso selection to not go trivial (always selecting certain variables, always not selecting certain variables, always making the correct selection), we also need to scale \\(\\lambda_n\\) as \\(\\lambda_n = \\lambda \\sqrt{n}\\).\nIf we take the subgradient of the objective, normalized by \\(1 / \\sqrt{n}\\), with respect to \\(\\theta_n\\), we get something like\n\\[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\nabla \\ell(\\theta_n; Y_n, X_n) - \\lambda s_n,\\]\nwhere \\(s_n\\) is the subgradient of the \\(L_1\\)-norm. This is the crucial step in Lee et al. (2013). For a the same set of variables selected and the same signs assigned, \\(\\lambda s_n\\) is a determined set. So what’s left is to relate the normalized score to the sufficient statistic.\nIn the asymptotic regime, the asymptotic sufficiency of score and the MLE means we can determine all the likelihood ratio, or equivalently, the entire sore function. From here we can approximate the score as a linear function at \\(0\\) as\n\\[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\nabla \\ell(\\theta_n; Y_n, X_n) \\approx \\left[\\frac{1}{n} \\sum_{i=1}^n \\nabla^2 \\ell(0; Y_n, X_n)\\right] \\beta,\\]\nor as a linear function based at the MLE \\(\\hat{\\theta}_n\\) (and hence \\(\\hat{\\beta}\\)),\n\\[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\nabla \\ell(\\theta_n; Y_n, X_n) \\approx \\left[\\frac{1}{n} \\sum_{i=1}^n \\nabla^2 \\ell(\\hat{\\theta}_n; Y_n, X_n)\\right] (\\beta - \\hat{\\beta}).\\]\nWe cannot however approximate this as a linear function at other points, such as the MLE restricted to the null hypothesis \\(\\Theta_0\\), as it reduces the degree of freedom.\nHow do we choose between these two approximation? In finite sample, the ‘data’ might not lie close to \\(0\\), rendering the first approximation ill-motivated. The second one has an appeal that it moves with the data and tends to approximate the score function better locally near the MLE.\nTo be more concrete, we can have a look at this in practice. We generated 1000 samples of 100 points from a logistic model and ran glmnet on each of the 100 samples. The unrestricted MLE is used as the statistic and plotted below. Colors follow the signs and the variables selected.\n\n\nShow the code\nsuppressPackageStartupMessages({\n  library(foreach)\n  library(doParallel)\n  library(glmnet)\n  library(ggplot2)\n  library(dplyr)\n})\n\ntheme_set(theme_minimal())\n\nLogistic <- function(z) {\n  return(exp(z) / (1 + exp(z)))\n}\n\nSignSummary <- function(x) {\n  return(paste(ifelse(x == 0, '0', ifelse(x > 0, '+', '-')), collapse = ''))\n}\n\nHessianLogistic <- function(x, theta) {\n  z <- x %*% theta\n  psi.2nd <- exp(z) / (1 + exp(z))^2\n  return(-t(x) %*% (x * as.vector(psi.2nd)) / nrow(x))\n}\n\nHessianGaussian <- function(x) {\n  return(-t(x) %*% x / nrow(x))\n}\n\nGLMSim <- function(theta, lambda, n, nRep = 1000, family = 'gaussian') {\n  # GLMSim produces a plot of the true selection events by simulation, with\n  # approximation of the selection event based on Taylor expansion of the\n  # log-likelihood at the true theta (dahsed) and the MLE (dotted)\n  #\n  # Args:\n  #   theta: true parameter\n  #   lambda: penalty parameter\n  #   n: sample size\n  #   nRep: number of points to be included\n  #   family: 'binomial' for logistic regression, 'gaussian' for linear\n  #     regression\n  #\n  # Returns:\n  #   A plot of the simulation\n  registerDoParallel(cores = 4)\n  \n  # generates nRep possible sets of n observations\n  x.cov.rt <- rbind(c(1, -0.1), c(-0.1, 1))\n  x <- matrix(rnorm(n * 2), n, 2) %*% x.cov.rt\n  if (family == 'binomial') {\n    pr <- Logistic(x %*% theta)\n    y <- foreach(i = 1:nRep, .combine = cbind) %dopar% {\n      set.seed(i)\n      rbinom(n, size = 1, prob = pr)\n    }\n  } else if (family == 'gaussian') {\n    mu <- x %*% theta\n    y <- foreach(i = 1:nRep, .combine = cbind) %dopar% {\n      set.seed(i)\n      rnorm(n, mean = mu)\n    }\n  } else {\n    stop('family is not binomial or gaussian')\n  }\n  \n  # runs glmnet to compute the selected variables at s = lambda\n  # computes mle as test statistic at s = 0\n  mle <- foreach(i = 1:nRep, .combine = rbind, .packages = \"glmnet\") %dopar% {\n    fit <- glmnet(x, y[, i], family = family, intercept = FALSE)\n    coeff <- as.matrix(\n      coef(\n        fit, s = c(0, lambda), exact = TRUE,\n        x = x, y = y[, i], family = family,\n        intercept = FALSE\n      )\n    )\n    selection <- SignSummary(coeff[-1, 2])\n    data.frame(\n      theta1 = coeff[2, 1], theta2 = coeff[3, 1],\n      selection = selection\n    )\n  }\n  mle$selection <- as.factor(mle$selection)\n  \n  # computes the selection event based on the MLE at the first row\n  if (family == 'binomial') {\n    hessian <- HessianLogistic(x, as.numeric(mle[1, 1:2]))\n  } else if (family == 'gaussian') {\n    hessian <- HessianGaussian(x)\n  }\n  \n  # computes the segments of the polyhedron for event '00'\n  zero.seg <- -t(solve(hessian, rbind(c(1, -1, -1, 1), c(1, 1, -1, -1)))) *\n    lambda\n  zero.seg <- cbind(zero.seg, zero.seg[c(2, 3, 4, 1), ])\n  zero.seg <- data.frame(zero.seg)\n  # adds segment representing the other events\n  zero.seg <- rbind(\n    zero.seg,\n    zero.seg %>% dplyr::mutate(X3 = X1, X4 = sign(X2) * 10),\n    zero.seg %>% dplyr::mutate(X4 = X2, X3 = sign(X1) * 10)\n  )\n  \n  # computes the selection event based on true theta\n  if (family == 'binomial') {\n    true.hessian <- HessianLogistic(x, theta)\n  } else if (family == 'gaussian') {\n    true.hessian <- hessian\n  }\n  \n  # computes the segments of the polyhedron for event '00'\n  true.zero.seg <- -t(solve(true.hessian, rbind(c(1, -1, -1, 1), c(1, 1, -1, -1)))) *\n    lambda\n  true.zero.seg <- cbind(true.zero.seg, true.zero.seg[c(2, 3, 4, 1), ])\n  true.zero.seg <- data.frame(true.zero.seg)\n  # adds segment representing the other events\n  true.zero.seg <- rbind(\n    true.zero.seg,\n    true.zero.seg %>%\n      dplyr::mutate(X3 = X1, X4 = sign(X2) * 10),\n    true.zero.seg %>%\n      dplyr::mutate(X4 = X2, X3 = sign(X1) * 10)\n  )\n  \n  ggplot(mle) +\n    geom_point(aes(x = theta1, y = theta2, color = selection)) +\n    geom_point(x = mle[1, 1], y = mle[1, 2], shape = 4) +\n    geom_segment(\n      aes(x = X1, y = X2, xend = X3, yend = X4),\n      data = zero.seg, color = 'black'\n    ) +\n    geom_segment(\n      aes(x = X1, y = X2, xend = X3, yend = X4),\n      data = true.zero.seg, color = 'black',\n      linetype = 'dashed'\n    ) +\n    coord_cartesian(\n      xlim = c(min(mle$theta1), max(mle$theta1)),\n      ylim = c(min(mle$theta2), max(mle$theta2))\n    )\n}\n\nGLMSim(\n  theta = c(0.05, 0), lambda = 0.04, n = 100, nRep = 1000,\n  family = 'binomial'\n)\n\n\n\n\n\n\n\n\n\nWe then look specifically at the sample marked with x. The approximating the selection event based on the score at zero will approximate the event much better around the origin, but we also care much less about this scenerio. The ‘high stake’ scanerio is when the statistic is close the the boundaries — and in these cases we would want the selection event to be approximated better for that section of the boundary. The MLE thus appeals to this.\nThe Hessian of the log-likelihood has to be approximated as well. The selection event given by the true Hessian is given as dashed lines above, while the estimated Hessian is given as solid lines. Notice that while the approximation on the left edge of the red region is not done well, the approximation is done well in the bottom edge, which is more important to us. Also notice that the estimated Hessian performs fairly well.\nFinally, how is these approximations linked to that of Taylor and Tibshirani (2017)? They used the lasso estimate with one extra Newton step as their test statistic. Assuming the log-likelihood behaves sufficiently quadratic, this is the same as using the MLE. We admit that their approach probably has a slight edge, an MLE would require solving a whole new approximation problem, while a one-extra-Newton-step lasso estimate is extremely easy to compute. In application, we believe these two methods should perform similarly.\n\n\n\n\nReferences\n\nLee, J. D., Sun, D. L., Sun, Y., and Taylor, J. E. (2016), “Exact post-selection inference, with application to the lasso,” The Annals of Statistics, 44, 907–927.\n\n\nTaylor, J. E., and Tibshirani, R. J. (2017), “Post-selection inference for \\(\\mathscr{l}\\)1-penalized likelihood models,” The Canadian Journal of Statistics, 19, 1212."
  },
  {
    "objectID": "posts/cal-fire.html",
    "href": "posts/cal-fire.html",
    "title": "Playing with Bayes and RStan",
    "section": "",
    "text": "I did not really much statistical training in my undergrad days, and my knowledge of statistics is pretty much confined to whatever grad level statistics classes Berkeley offered — 99% of those was frequentist — so I lack the Bayesian exposure that most statistics undergrad would have received. So when something slightly Bayesian (does empirical Bayes count?) showed up, I decided to teach myself using Gelman et al. (2013).\nIt is hard to learn something new without any examples, and I happen to stumble upon this tweet:\n\n\nTrend line can be misleading, and there are good years too. But certainly it looks like bad years had become worse. Just curious, where can I find the data itself for educational purposes?\n\n— Kenneth Hung ((kenhungkk?)) September 7, 2020\n\n\nThe data itself is here and I got to learn how to handle PDFs with pdftools as well.\n\nlibrary(pdftools)\n\nUsing poppler version 22.08.0\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(rstan)\n\nLoading required package: StanHeaders\n\n\nrstan (Version 2.21.5, GitRev: 2e1f913d3ca3)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\n\n\n\nAttaching package: 'rstan'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(foreach)\nlibrary(doParallel)\n\nLoading required package: iterators\n\n\nLoading required package: parallel\n\ntheme_set(theme_minimal())\noptions(mc.cores = parallel::detectCores())\nregisterDoParallel(cores = parallel::detectCores())\n\nraw.text <- pdf_text(\n  'https://www.fire.ca.gov/media/11397/fires-acres-all-agencies-thru-2018.pdf'\n) %>%\n  str_split(\"\\n\") %>%\n  unlist()\n\ndata <- raw.text[4:35] %>%\n  as_tibble(.name_repair = \"unique\") %>%\n  mutate(\n    full.year = str_sub(value, end = 6), acres = str_sub(value, start = 132)\n  ) %>%\n  select(-value) %>%\n  mutate_all(str_trim) %>%\n  mutate_all(str_replace_all, \",\", \"\") %>%\n  mutate_all(as.numeric) %>%\n  mutate(year = full.year - min(full.year))\nN <- nrow(data)\n\nggplot(data) + geom_line(aes(x = full.year, y = acres))\n\n\n\n\n\n\n\n\nLike I said in the tweet, a linear fit seems off.\nUsing the default priors, I ran a Bayesian linear regression. I have to say, seeing the chains running and utilizing my new laptop’s computation power was very exciting.\n\nmodel.stan <- \"\ndata {\n  int<lower=0> N;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(alpha + beta * x, sigma);\n}\n\"\n\nfit <- stan(\n  model_code = model.stan,\n  data = list(N = N, x = data$year, y = data$acres),\n  iter = 5000\n)\n\nsummary(fit)$summary\n\n             mean      se_mean           sd        2.5%         25%         50%\nalpha 240006.8645 2.539418e+03 1.551917e+05 -68013.3021 137908.1746 239675.9123\nbeta   25350.8491 1.403148e+02 8.616489e+03   8211.8978  19674.1566  25370.7932\nsigma 447205.8818 8.786446e+02 6.045076e+04 348075.7837 404490.9808 440528.5524\nlp__    -418.5083 2.070267e-02 1.255783e+00   -421.7157   -419.1005   -418.1869\n              75%       97.5%    n_eff      Rhat\nalpha 344199.6611 552040.4477 3734.813 0.9997395\nbeta   31137.5510  42277.3893 3770.976 0.9999660\nsigma 481952.3839 586203.5716 4733.442 1.0006884\nlp__    -417.5703   -417.0499 3679.393 1.0005287\n\n\nSince all Bayesians do is do posterior draws, I don’t find it hard to understand the result. But what matters the most to me is that the data is fit well. As BDA would suggest, I should do a posterior predictive check, specifically something that would demonstrate my suspicion that linear model isn’t a good fit. By looking at the time series, I would guess that there are fewer positive residuals than negative ones. So I used the proportion of positive residuals after OLS as the test statistic. Embarrassingly it took me a long time to realize how to do a posterior predictive check for regression, the most basic example in Chapter 14 surprisingly did not emphasize this part.\n\npost <- extract(fit)\npost.pred.stat <- foreach(\n  alpha = post$alpha,\n  beta = post$beta,\n  sigma = post$sigma,\n  .combine = \"c\"\n) %dopar% {\n  y.rep <- alpha + beta * data$year + sigma * rnorm(N)\n  residual.rep <- residuals(lm(y.rep ~ data$year))\n  mean(residual.rep > 0)\n}\n\nresidual <- residuals(lm(acres ~ year, data = data))\nobs.stat <- mean(residual > 0)\n\nggplot() +\n  geom_histogram(\n    aes(x = post.pred.stat), alpha = 0.5, binwidth = 1 / nrow(data)\n  ) +\n  geom_vline(xintercept = obs.stat)\n\n\n\n\n\n\n\n\nIt looks like we have way too few positive residuals and I should probably use a log-linear model.\n\nmodel.stan <- \"\ndata {\n  int<lower=0> N;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(alpha + beta * x, sigma);\n}\n\"\n\nfit <- stan(\n  model_code = model.stan,\n  data = list(N = N, x = data$year, y = log(data$acres)),\n  iter = 5000\n)\n\nsummary(fit)$summary\n\n             mean      se_mean         sd         2.5%         25%        50%\nalpha 12.41908326 0.0043276351 0.27139695  11.89028656 12.23991697 12.4138605\nbeta   0.04168707 0.0002393191 0.01502463   0.01160145  0.03212815  0.0418301\nsigma  0.76934629 0.0015107737 0.10299026   0.60040099  0.69765468  0.7585032\nlp__  -7.16169509 0.0235539924 1.33780557 -10.65728017 -7.71969447 -6.8090985\n              75%       97.5%    n_eff     Rhat\nalpha 12.59937319 12.95217175 3932.861 1.000875\nbeta   0.05154402  0.07100689 3941.421 1.000715\nsigma  0.82879300  0.99799036 4647.222 1.000257\nlp__  -6.20961693 -5.68965437 3225.945 1.000177\n\n\nAnd we perform the same check to see if the residuals are symmetric.\n\npost <- extract(fit)\npost.pred.stat <- foreach(\n  alpha = post$alpha,\n  beta = post$beta,\n  sigma = post$sigma,\n  .combine = \"c\"\n) %dopar% {\n  y.rep <- alpha + beta * data$year + sigma * rnorm(N)\n  residual.rep <- residuals(lm(y.rep ~ data$year))\n  mean(residual.rep > 0)\n}\n\nresidual <- residuals(lm(log(acres) ~ year, data = data))\nobs.stat <- mean(residual > 0)\n\nggplot() +\n  geom_histogram(\n    aes(x = post.pred.stat), alpha = 0.5, binwidth = 1 / nrow(data)\n  ) +\n  geom_vline(xintercept = obs.stat)\n\n\n\n\n\n\n\n\nMuch better! Of course the model is not going to be correct, but we just need to keep checking for statistics that we care about. One idea I had is the number of times a new record is set. From the time series, it is 5 — we will count the very first year, not that it really matters. I thought this may be revealing if there is a lot of autocorrelation in the time series — for example, the more acres are burnt the previous year, the less there is to burn the year after.\n\npost <- extract(fit)\npost.pred.stat <- foreach(\n  alpha = post$alpha,\n  beta = post$beta,\n  sigma = post$sigma,\n  .combine = \"c\"\n) %dopar% {\n  y.rep <- alpha + beta * data$year + sigma * rnorm(N)\n  sum(y.rep == cummax(y.rep))\n}\n\nobs.stat <- sum(data$acres == cummax(data$acres))\n\nggplot() +\n  geom_histogram(\n    aes(x = post.pred.stat), alpha = 0.5, binwidth = 1\n  ) +\n  geom_vline(xintercept = obs.stat)\n\n\n\n\n\n\n\n\nNot too bad! For both model it looks like the slope is positive. There are many other data that would have been relevant to this analysis, such as the rainfall the year before and other climate data. There are also more sophisticated things such as Bayesian ARIMA that I could do (but I don’t know how), but hey, there are only 32 points in this dataset.\n\n\n\n\nReferences\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013), Bayesian Data Analysis."
  },
  {
    "objectID": "posts/binomial-ranking.html",
    "href": "posts/binomial-ranking.html",
    "title": "Binomial ranking with SARS data",
    "section": "",
    "text": "With the coronavirus spreading to many countries, Rebecca asked me a curious question: how does the US perform during SARS compared to other regions in terms of survival rate? While we can compute the survival rate of all infected regions and rank them accordingly, we are ignoring the sampling variability. For example, South Africa that has one case but also one death, does not necessarily perform worse than Indonesia where there were two cases but both patients survived."
  },
  {
    "objectID": "posts/binomial-ranking.html#setup",
    "href": "posts/binomial-ranking.html#setup",
    "title": "Binomial ranking with SARS data",
    "section": "Setup",
    "text": "Setup\nOf course there are many other factors, but we can consider an idealized model where the number of patients, \\(n_i\\) in each country is predetermined, but the number of deaths, \\(X_i\\) is random and comes from a binomial draw: \\[X_i \\sim \\text{Binomial}(n_i, p_i),\\] where \\(p_i\\) represent the chance of a patient dying in region \\(i\\). We would then want to provide simultaneous confidence intervals for the rank of region \\(i\\), \\(r_i\\), as defined in Al Mohamad et al. (2022): \\[r_i = 1 + \\#\\{j \\ne i: p_j < p_i\\}\\] There is an obvious Bayesian way to achieve this. By setting up a reasonable prior, we can perform posterior draws of \\(p_i\\) and search for confidence intervals of ranks that covers \\((1 - \\alpha)\\) of the posterior draws. Naturally this can be extended to an empirical Bayes way as well, as suggested in one of the comments in this Cross Validated thread. We want to focus on strict frequentist methods here."
  },
  {
    "objectID": "posts/binomial-ranking.html#data",
    "href": "posts/binomial-ranking.html#data",
    "title": "Binomial ranking with SARS data",
    "section": "Data",
    "text": "Data\nI have never done data scraping, so I am glad that this led me to learn rvest. We read in the table from the SARS page on Wikipedia. It looks like this:\n\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(ggplot2)\n  library(kableExtra)\n  library(rvest)\n  library(tidyr)\n})\n\ndata <- \"https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome\" %>%\n  read_html %>%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div/table[2]') %>%\n  html_table(fill = TRUE)\ndata <- data[[1]] %>%\n  setNames(c('region', 'cases', 'deaths', 'fatality', 'X1')) %>%\n  select(region, cases, deaths) %>%\n  filter(!grepl('total', tolower(region)), !grepl('\\\\^', region)) %>%\n  mutate(\n    region = trimws(gsub('\\\\[[[:print:]]\\\\]', '', region)),\n    cases = as.numeric(gsub(',', '', cases)),\n    deaths = as.numeric(gsub(',', '', deaths)),\n    fatality = deaths / cases\n  )\ndata %>% head() %>% kbl(format = \"markdown\")\n\n\n\n\nregion\ncases\ndeaths\nfatality\n\n\n\n\nChina\n5327\n349\n0.0655153\n\n\nHong Kong\n1755\n299\n0.1703704\n\n\nTaiwan\n346\n81\n0.2341040\n\n\nCanada\n251\n43\n0.1713147\n\n\nSingapore\n238\n33\n0.1386555\n\n\nVietnam\n63\n5\n0.0793651"
  },
  {
    "objectID": "posts/binomial-ranking.html#method-1-simultaneous-confidence-intervals",
    "href": "posts/binomial-ranking.html#method-1-simultaneous-confidence-intervals",
    "title": "Binomial ranking with SARS data",
    "section": "Method 1: Simultaneous confidence intervals",
    "text": "Method 1: Simultaneous confidence intervals\nOne way is to construct simultaneous confidence intervals for each of the region, and “project” to figure out the ranks. We implement those here:\n\n\nShow the code\numpu.expfam.test <- function(x, prob, u = runif(1)) {\n  x <- x - min(which(prob != 0)) + 1\n  prob <- prob[min(which(prob != 0)):max(which(prob != 0))]\n  n <- length(prob)\n  if (x > n | x <= 0) {\n    return(0)\n  }\n  \n  mean.x <- sum(prob * 1:n)\n  # observation is mean\n  if (abs(x - mean.x) < .Machine$double.eps^0.5) {\n    return(1 - u * prob[x])\n  }\n  # observation is on lower tail\n  if (x < mean.x) {\n    x <- n + 1 - x\n    mean.x <- n + 1 - mean.x\n    prob <- rev(prob)\n  }\n  \n  # dot product with this vector gives the covariance with x\n  cov.vec <- prob * (1:n - mean.x)\n  \n  prob.hi <- sum(prob[-(1:x)]) + prob[x] * u\n  cov.tail <- sum(cov.vec[-(1:x)]) + cov.vec[x] * u\n  cov.cumsum <- cumsum(cov.vec) + cov.tail\n  lo <- min(which(cov.cumsum < 0))\n  prob.lo <- sum(prob[1:lo]) - cov.cumsum[lo] / cov.vec[lo] * prob[lo]\n  \n  prob.lo + prob.hi\n}\n\numpu.binom.test <- function(x, n, p, u = runif(1)) {\n  umpu.expfam.test(x + 1, dbinom(0:n, n, p), u)\n}\n\numau.binom.ci <- function(x, n, alpha, u = runif(1)) {\n  f <- function(p) {\n    umpu.binom.test(x, n, p, u) - alpha\n  }\n  tol <- .Machine$double.eps^0.5\n  if (x == 0) {\n    ci.lo <- 0\n  } else {\n    ci.lo <- uniroot(f, c(0, x / n), tol = tol)$root\n  }\n  if (x == n) {\n    ci.hi <- 1\n  } else {\n    ci.hi <- uniroot(f, c(x / n, 1), tol = tol)$root\n  }\n  c(ci.lo, ci.hi)\n}\n\n# delta is the difference in log-odds\numpu.binom.contrast.test <- function(x1, n1, x2, n2, delta = 0, u = runif(1)) {\n  log.prob <- dbinom(0:(x1 + x2), n1, 0.5, log = TRUE) +\n    dbinom((x1 + x2):0, n2, 0.5, log = TRUE) +\n    0:(x1 + x2) * delta\n  log.prob <- log.prob - max(log.prob)\n  prob <- exp(log.prob)\n  prob <- prob / sum(prob)\n  umpu.expfam.test(x1 + 1, prob, u)\n}\n\n\nWe construct UMAU simultaneous confidence intervals:\n\nset.seed(20200215)\n\ndata.ci <- mapply(\n  umau.binom.ci,\n  data$deaths,\n  data$cases,\n  MoreArgs = list(alpha = 0.05 / nrow(data))\n) %>%\n  t %>%\n  data.frame %>%\n  setNames(c('ci.lo', 'ci.hi'))\ndata <- data %>% cbind(data.ci)\n\nggplot(\n  data,\n  aes(x = factor(region, region), y = fatality, ymin = ci.lo, ymax = ci.hi)\n) +\n  geom_point() +\n  geom_errorbar() +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(x = \"Region\", y = \"Fatality\")\n\n\n\n\n\n\n\n\nThese UMAU intervals are random by nature. Furthermore, the simultaneous coverage is achieved by Bonferroni correction here. A more fine-tuned analysis can be obtain by strategically distribute the type I error over the 29 regions, especially when some of these intervals are likely uninformative.\nWe can now compute the ranks for all parameters falling into the simultaneous confidence region\n\ndata %>%\n  mutate(\n    rank.lo = 1 + rowSums(outer(ci.lo, ci.hi, FUN = '-') >= 0),\n    rank.hi = rowSums(outer(ci.hi, ci.lo, FUN = '-') >= 0)\n  ) %>%\n  kbl(format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nregion\ncases\ndeaths\nfatality\nci.lo\nci.hi\nrank.lo\nrank.hi\n\n\n\n\nChina\n5327\n349\n0.0655153\n0.0553309\n0.0767850\n1\n27\n\n\nHong Kong\n1755\n299\n0.1703704\n0.1431314\n0.2001229\n2\n31\n\n\nTaiwan\n346\n81\n0.2341040\n0.1675014\n0.3108369\n2\n31\n\n\nCanada\n251\n43\n0.1713147\n0.1038446\n0.2564828\n2\n31\n\n\nSingapore\n238\n33\n0.1386555\n0.0791824\n0.2173352\n2\n31\n\n\nVietnam\n63\n5\n0.0793651\n0.0097358\n0.2311465\n1\n31\n\n\nUnited States\n27\n0\n0.0000000\n0.0000000\n0.1884739\n1\n31\n\n\nPhilippines\n14\n2\n0.1428571\n0.0001477\n0.5865567\n1\n31\n\n\nThailand\n9\n2\n0.2222222\n0.0012364\n0.7285998\n1\n31\n\n\nGermany\n9\n0\n0.0000000\n0.0000000\n0.5160983\n1\n31\n\n\nMongolia\n9\n0\n0.0000000\n0.0000000\n0.4660410\n1\n31\n\n\nFrance\n7\n1\n0.1428571\n0.0000000\n0.7410531\n1\n31\n\n\nAustralia\n6\n0\n0.0000000\n0.0000000\n0.5821487\n1\n31\n\n\nMalaysia\n5\n2\n0.4000000\n0.0022369\n0.9623212\n1\n31\n\n\nSweden\n5\n0\n0.0000000\n0.0000000\n0.7923318\n1\n31\n\n\nUnited Kingdom\n4\n0\n0.0000000\n0.0000000\n0.7418809\n1\n31\n\n\nItaly\n4\n0\n0.0000000\n0.0000000\n0.8810932\n1\n31\n\n\nBrazil\n3\n0\n0.0000000\n0.0000000\n0.8873430\n1\n31\n\n\nIndia\n3\n0\n0.0000000\n0.0000000\n0.9577680\n1\n31\n\n\nSouth Korea\n3\n0\n0.0000000\n0.0000000\n0.9277255\n1\n31\n\n\nIndonesia\n2\n0\n0.0000000\n0.0000000\n0.9982266\n1\n31\n\n\nSouth Africa\n1\n1\n1.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nColombia\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nKuwait\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nIreland\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nMacao\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nNew Zealand\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nRomania\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nRussia\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nSpain\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31\n\n\nSwitzerland\n1\n0\n0.0000000\n0.0000000\n1.0000000\n1\n31"
  },
  {
    "objectID": "posts/binomial-ranking.html#method-2-simultaneous-pairwise-tests",
    "href": "posts/binomial-ranking.html#method-2-simultaneous-pairwise-tests",
    "title": "Binomial ranking with SARS data",
    "section": "Method 2: Simultaneous pairwise tests",
    "text": "Method 2: Simultaneous pairwise tests\nBinomial distributions is an exponential family, so contrasts like \\(p_i - p_j\\) are amenable to UMPU tests. We can perform \\(\\binom{29}{2}\\) pairwise tests, with Bonferroni correction, and draw conclusions about the ranks.\n\nset.seed(113013)\n\npairwise.test <- by(\n  expand.grid(region1 = 1:nrow(data), region2 = 1:nrow(data)),\n  1:(nrow(data)^2),\n  function(pair) {\n    if (pair$region1 == pair$region2) {\n      return(1)\n    }\n    umpu.binom.contrast.test(\n      data$deaths[pair$region1],\n      data$cases[pair$region1],\n      data$deaths[pair$region2],\n      data$cases[pair$region2],\n      delta = 0\n    )\n  }\n) %>%\n  matrix(nrow(data), nrow(data))\npairwise.test[upper.tri(pairwise.test)] <- t(\n  pairwise.test\n)[upper.tri(pairwise.test)]\n\ndata %>%\n  select(-ci.lo, -ci.hi) %>%\n  mutate(\n    rank.lo = 1 +\n      rowSums(\n        (pairwise.test * choose(nrow(data), 2) < 0.05) &\n          (outer(data$fatality, data$fatality, FUN = \"-\") > 0)\n      ),\n    rank.hi = nrow(data) -\n      rowSums(\n        (pairwise.test * choose(nrow(data), 2) < 0.05) &\n          (outer(fatality, fatality, FUN = \"-\") < 0)\n      )\n  ) %>%\n  kbl(format = \"markdown\")\n\n\n\n\nregion\ncases\ndeaths\nfatality\nrank.lo\nrank.hi\n\n\n\n\nChina\n5327\n349\n0.0655153\n1\n28\n\n\nHong Kong\n1755\n299\n0.1703704\n2\n31\n\n\nTaiwan\n346\n81\n0.2341040\n2\n31\n\n\nCanada\n251\n43\n0.1713147\n2\n31\n\n\nSingapore\n238\n33\n0.1386555\n1\n31\n\n\nVietnam\n63\n5\n0.0793651\n1\n31\n\n\nUnited States\n27\n0\n0.0000000\n1\n31\n\n\nPhilippines\n14\n2\n0.1428571\n1\n31\n\n\nThailand\n9\n2\n0.2222222\n1\n31\n\n\nGermany\n9\n0\n0.0000000\n1\n31\n\n\nMongolia\n9\n0\n0.0000000\n1\n31\n\n\nFrance\n7\n1\n0.1428571\n1\n31\n\n\nAustralia\n6\n0\n0.0000000\n1\n31\n\n\nMalaysia\n5\n2\n0.4000000\n1\n31\n\n\nSweden\n5\n0\n0.0000000\n1\n31\n\n\nUnited Kingdom\n4\n0\n0.0000000\n1\n31\n\n\nItaly\n4\n0\n0.0000000\n1\n31\n\n\nBrazil\n3\n0\n0.0000000\n1\n31\n\n\nIndia\n3\n0\n0.0000000\n1\n31\n\n\nSouth Korea\n3\n0\n0.0000000\n1\n31\n\n\nIndonesia\n2\n0\n0.0000000\n1\n31\n\n\nSouth Africa\n1\n1\n1.0000000\n1\n31\n\n\nColombia\n1\n0\n0.0000000\n1\n31\n\n\nKuwait\n1\n0\n0.0000000\n1\n31\n\n\nIreland\n1\n0\n0.0000000\n1\n31\n\n\nMacao\n1\n0\n0.0000000\n1\n31\n\n\nNew Zealand\n1\n0\n0.0000000\n1\n31\n\n\nRomania\n1\n0\n0.0000000\n1\n31\n\n\nRussia\n1\n0\n0.0000000\n1\n31\n\n\nSpain\n1\n0\n0.0000000\n1\n31\n\n\nSwitzerland\n1\n0\n0.0000000\n1\n31\n\n\n\n\n\nHere we are bounded to correct for all \\(\\binom{29}{2}\\) tests, instead of taking advantage of Tukey’s HSD, which incurs a much smaller penalty for multiple testing. Asymptotically we can always think of the likelihood as if it came from a Gaussian distribution and use Tukey’s HSD, but we most definitely are not in any reasonable asymptotic regime here."
  },
  {
    "objectID": "posts/binomial-ranking.html#thoughts",
    "href": "posts/binomial-ranking.html#thoughts",
    "title": "Binomial ranking with SARS data",
    "section": "Thoughts",
    "text": "Thoughts\nAre there better, strictly frequentist methods for computing these rank confidence intervals? This alone seems hard, but there is a natural, even harder generalization of this problem: suppose we have a joint distribution of exponential families where the base measure does not need to be the same \\[p_i(X_i; \\theta_i) = h(x_i) \\exp(\\theta_i x_i - A_i(\\theta_i)),\\] is there a powerful method for ranking \\(\\theta_i\\)?"
  },
  {
    "objectID": "posts/nudge-meta-analysis.html",
    "href": "posts/nudge-meta-analysis.html",
    "title": "Meta-analysis on nudging experiments",
    "section": "",
    "text": "As usual, I am not a psychologist, and do not even run experiments myself, so the model I am discussing here is going to simplistic. But if we are willing to assume a very simple publication bias model, where only statistically significant results are published (a.k.a. “file-drawer effect”), then we can apply the methods from my paper with Will (Hung and Fithian 2020), and answer questions such as\n\nDoes any “nudging” experiment have real effect? How many of the experiments?\nWhat happens if we counter the file-drawer effect with a more stringent threshold?\n\nSo first of all, we are going to only look at p-values that are smaller than \\(0.05\\), and adjust them by division by \\(0.05\\), as in our aforementioned paper, or Zhao et al. (2018).\n\n\nShow the code\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(tidyr)\n})\n\nnudge_rawdata <- read.csv(\"https://osf.io/ubt9a/download/\")\n\nnudge_data <- nudge_rawdata %>%\n  select(publication_id, study_id, es_id, cohens_d, variance_d) %>%\n  mutate(\n    est = cohens_d,\n    sd = sqrt(variance_d),\n    z = est / sd,\n    p = 2 * pnorm(-abs(z))\n  )\n\nalpha <- 0.05\nadj_p <- nudge_data$p[nudge_data$p < alpha] / alpha\n\nhist(\n  adj_p, main = \"Histogram of adjusted p-values\", xlab = \"adjusted p-values\"\n)\n\n\n\n\n\n\n\n\n\nThe first question can be answered by our method, and for convenience, we will just pick \\(\\lambda\\) to be \\(0.5\\). All adjusted p-values will fall in one of these categories:\n\n\n\n\nNull\nNon-null\nTotal\n\n\n\n\nSmall (\\(p \\le 0.5\\))\n*\n*\n*\n\n\nBig (\\(p > 0.5\\))\n\\(U\\)\n*\n\\(B\\)\n\n\nTotal\n\\(V\\)\n*\n\\(R\\)\n\n\n\nSpecifically, we want to make inference on the false discovery proportion (FDP), which is \\(V / R\\), since every experiment was deemed a discovery beofre. Equivalently, \\(1 - V/R\\) would indicate the proportion of experiments that has an actual effect. Intuitively, if \\(B\\) is not that large, \\(U\\) cannot be large and \\(V\\) cannot be so big either. Formally, we will be using the inequality (2) from the paper:\n\\[B \\ge U \\ge_{\\text{st}} \\ge \\text{Binomial}(V, 1 - \\lambda),\\]\nand rejecting unlikely large values of \\(V\\). As a side, we can also get a point estimate of the FDP.\n\ninternal.comparison <- function(pval, lambda) {\n  R <- length(pval)\n  B <- sum(pval > lambda)\n  V_est <- B / (1 - lambda)\n  V <- 0:R\n  V_ucb <- V[max(which(pbinom(B, size = V, prob = 1 - lambda) > 0.05))]\n  data.frame(\n    R = R,\n    V_est = V_est,\n    V_ucb = V_ucb,\n    FDP_est = V_est / R,\n    FDP_ucb = V_ucb / R\n  )\n}\n\ninternal.comparison(adj_p, 0.5)\n\n    R V_est V_ucb   FDP_est   FDP_ucb\n1 269    74    90 0.2750929 0.3345725\n\n\nIn other words, at 95% confidence, at most 90 of the 269 are noise, meaning 179 of them have actual nonzero effect — but of course whether the effect is big enough to be meaningful is not up to me.\nWould these numbers look better if we use a more strigent threshold? It just so happens that our paper also has a method for it!\nWe will not be using all p-values this time, but for the used ones, they will fall into one of these cells in this table:\n\n\n\n\nNull\nNon-null\nTotal\n\n\n\n\nSmall (\\(p \\le \\alpha / 0.05\\))\n\\(V_\\alpha\\)\n\\(T_\\alpha\\)\n\\(R_\\alpha\\)\n\n\nBig (\\(p > 0.5\\))\n\\(U\\)\n\\(W\\)\n\\(B\\)\n\n\nTotal\n\\(N_0\\)\n*\n\\(N\\)\n\n\n\nThis time the inequality is more involved, from Lemma 1 in the paper (Did we make a typo in there?):\n\\[B = U + W \\ge_{\\text{st}} \\text{Binomial}\\left(N_0, \\frac{\\alpha / 0.05}{\\alpha / 0.05 + 0.5}\\right) + W \\ge_{\\text{st}} \\text{Binomial}(N - T_\\alpha, \\beta).\\]\nBut to use it, it is the same: we reject large values of \\(V_\\alpha\\) on the basis of small values of \\(B\\), and get a point estimate for this on the side.\n\n\nShow the code\nexternal.comparison <- function(pval, lambda, alpha) {\n  alpha <- alpha / 0.05\n  stopifnot(lambda > alpha)\n  beta <- (1 - lambda) / (1 - lambda + alpha)\n  R_alpha <- sum(pval < alpha)\n  B <- sum(pval > lambda)\n  V_alpha_est <- (1 - beta) / beta * B\n  N <- R_alpha + B\n  Qs <- 0:N\n  Q <- Qs[\n    max(which(pbinom(B, size = Qs, prob = beta) > 0.05))\n  ]\n  V_alpha_ucb <- Q - B\n  data.frame(\n    alpha = alpha * 0.05,\n    R = R_alpha,\n    V_est = V_alpha_est,\n    V_ucb = V_alpha_ucb,\n    FDP_est = V_alpha_est / R_alpha,\n    FDP_ucb = V_alpha_ucb / R_alpha\n  )\n}\n\nrbind(\n  external.comparison(adj_p, 0.5, 0.01),\n  external.comparison(adj_p, 0.5, 0.005),\n  external.comparison(adj_p, 0.5, 0.001)\n)\n\n\n  alpha   R V_est V_ucb    FDP_est    FDP_ucb\n1 0.010 195 14.80    23 0.07589744 0.11794872\n2 0.005 174  7.40    13 0.04252874 0.07471264\n3 0.001 127  1.48     4 0.01165354 0.03149606\n\n\n\n\n\n\nReferences\n\nHung, K., and Fithian, W. (2020), “Statistical Methods for Replicability Assessment,” The Annals of Applied Statistics, 14, 1063–1087. https://doi.org/10.1214/20-AOAS1336.\n\n\nZhao, Q., Small, D. S., and Su, W. J. (2018), “Multiple testing when many p-values are uniformly conservative, with application to testing qualitative interaction in educational interventions,” Journal of the American Statistical Association."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kenneth Hung",
    "section": "",
    "text": "I am a statistician in the Core Data Science Team at Meta. I am working on experimentation-related statistical questions and causal inference.\nI received my Ph.D. in mathematics at UC Berkeley, under Will Fithian. I focused on conditional inference and post-selection inference, as well as scientific replicability and mathematical statistics. My qualifying exam was on August 11 2016. The syllabus can be found here (pdf, tex).\nPrior to the Ph.D., I completed a B.S. with honors in mathematics with a computer science minor at Caltech. I was born in Toronto, Ontario and spent my formative years in Hong Kong and California.\nMy résumé and my c.v. are available here and here."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Parametric mean estimation\n\n\n\n\n\n\n\napplied statistics\n\n\ndata science\n\n\n\n\nA common task in data science is mean estimation. The go-to estimator is sample mean, which is non-parametric in nature. But what if we have good reasons to believe in a parametric model for the population?\n\n\n\n\n\n\nMar 11, 2023\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\nPhilosophy of statistics, or data science\n\n\n\n\n\n\n\ndata science\n\n\n\n\nThis is an ambitious title for a post, but perhaps, and I hope, every data scientist or statistician eventually has their own philosophy on what we are doing.\n\n\n\n\n\n\nSep 10, 2022\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\nMeta-analysis on nudging experiments\n\n\n\n\n\n\n\nmultiple testing\n\n\n\n\nA recent meta-analysis on choice architecture, or “nudging”, but people do not seem to agree on if “nudging” works.\n\n\n\n\n\n\nAug 27, 2022\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\nMoment conditions heuristics\n\n\n\n\n\n\n\napplied statistics\n\n\n\n\nMoment conditions are common in literature, but how do we assess if they reasonably apply?\n\n\n\n\n\n\nApr 11, 2022\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\nPlaying with Bayes and RStan\n\n\n\n\n\n\n\nstatistical computing\n\n\n\n\nFirst time pretending to be a Bayesian, and trying RStan.\n\n\n\n\n\n\nSep 25, 2020\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\nNormal distribution in Python\n\n\n\n\n\n\n\nstatistical computing\n\n\n\n\nGetting the hang of statistical computing in Python\n\n\n\n\n\n\nAug 4, 2020\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\nBinomial ranking with SARS data\n\n\n\n\n\n\n\nmultiple testing\n\n\n\n\nIf deaths in each country follows a binomial distribution, how do we rank them by the probability parameter?\n\n\n\n\n\n\nFeb 15, 2020\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\np-value screening\n\n\n\n\n\n\n\nmultiple testing\n\n\nselective inference\n\n\n\n\nBetter multiple testing by screening p-values first, to reduce the penalty in common multiple testing procedure\n\n\n\n\n\n\nSep 21, 2018\n\n\nKenneth Hung\n\n\n\n\n\n\n\n\nL1-penalized likelihood asymptotics\n\n\n\n\n\n\n\nselective inference\n\n\n\n\nGeneralizing Lasso penalty to non-linear model, e.g. a binomial model.\n\n\n\n\n\n\nApr 10, 2017\n\n\nKenneth Hung\n\n\n\n\n\n\nNo matching items"
  }
]